{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages from Keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import required packages for plotting\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "import matplotlib.patches as mpatches\n",
    "# import the function for plotting decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.read_csv(r'C:\\Users\\My PC\\Desktop\\Machine Learning\\Data Engineer\\outlier_feats.csv')\n",
    "target = pd.read_csv(r'C:\\Users\\My PC\\Desktop\\Machine Learning\\Data Engineer\\outlier_target.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size =  (3359, 2)\n",
      "Y size =  (3359, 1)\n",
      "Number of examples =  3359\n"
     ]
    }
   ],
   "source": [
    "print(\"X size = \", feats.shape)\n",
    "print(\"Y size = \", target.shape)\n",
    "print(\"Number of examples = \", feats.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Feature 2')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_1=plt.scatter(feats.loc[target['Class']==0,'feature1'], feats.loc[target['Class']==0,'feature2'], c=\"red\", s=40, edgecolor='k')\n",
    "class_2=plt.scatter(feats.loc[target['Class']==1,'feature1'], feats.loc[target['Class']==1,'feature2'], c=\"blue\", s=40, edgecolor='k')\n",
    "plt.legend((class_1, class_2),('Fail','Pass'))\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid', input_dim=2)) \n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2687 samples, validate on 672 samples\n",
      "Epoch 1/100\n",
      "2687/2687 [==============================] - 4s 1ms/step - loss: 0.2127 - val_loss: 0.1902\n",
      "Epoch 2/100\n",
      "2687/2687 [==============================] - 2s 829us/step - loss: 0.1654 - val_loss: 0.1655\n",
      "Epoch 3/100\n",
      "2687/2687 [==============================] - 2s 595us/step - loss: 0.1469 - val_loss: 0.1504\n",
      "Epoch 4/100\n",
      "2687/2687 [==============================] - 1s 509us/step - loss: 0.1343 - val_loss: 0.1386\n",
      "Epoch 5/100\n",
      "2687/2687 [==============================] - 1s 450us/step - loss: 0.1240 - val_loss: 0.1286\n",
      "Epoch 6/100\n",
      "2687/2687 [==============================] - 1s 473us/step - loss: 0.1151 - val_loss: 0.1198\n",
      "Epoch 7/100\n",
      "2687/2687 [==============================] - 1s 454us/step - loss: 0.1073 - val_loss: 0.1120\n",
      "Epoch 8/100\n",
      "2687/2687 [==============================] - 1s 457us/step - loss: 0.1003 - val_loss: 0.1051\n",
      "Epoch 9/100\n",
      "2687/2687 [==============================] - 1s 425us/step - loss: 0.0940 - val_loss: 0.0990\n",
      "Epoch 10/100\n",
      "2687/2687 [==============================] - 1s 449us/step - loss: 0.0884 - val_loss: 0.0934\n",
      "Epoch 11/100\n",
      "2687/2687 [==============================] - 1s 437us/step - loss: 0.0834 - val_loss: 0.0885\n",
      "Epoch 12/100\n",
      "2687/2687 [==============================] - 1s 438us/step - loss: 0.0788 - val_loss: 0.0840\n",
      "Epoch 13/100\n",
      "2687/2687 [==============================] - 1s 445us/step - loss: 0.0747 - val_loss: 0.0799\n",
      "Epoch 14/100\n",
      "2687/2687 [==============================] - 1s 441us/step - loss: 0.0709 - val_loss: 0.0762\n",
      "Epoch 15/100\n",
      "2687/2687 [==============================] - 1s 430us/step - loss: 0.0675 - val_loss: 0.0729\n",
      "Epoch 16/100\n",
      "2687/2687 [==============================] - 1s 429us/step - loss: 0.0644 - val_loss: 0.0698\n",
      "Epoch 17/100\n",
      "2687/2687 [==============================] - 1s 450us/step - loss: 0.0615 - val_loss: 0.0670\n",
      "Epoch 18/100\n",
      "2687/2687 [==============================] - 1s 467us/step - loss: 0.0589 - val_loss: 0.0645\n",
      "Epoch 19/100\n",
      "2687/2687 [==============================] - 1s 448us/step - loss: 0.0565 - val_loss: 0.0621\n",
      "Epoch 20/100\n",
      "2687/2687 [==============================] - 1s 441us/step - loss: 0.0542 - val_loss: 0.0599\n",
      "Epoch 21/100\n",
      "2687/2687 [==============================] - 1s 427us/step - loss: 0.0522 - val_loss: 0.0579\n",
      "Epoch 22/100\n",
      "2687/2687 [==============================] - 1s 428us/step - loss: 0.0503 - val_loss: 0.0561\n",
      "Epoch 23/100\n",
      "2687/2687 [==============================] - 1s 429us/step - loss: 0.0485 - val_loss: 0.0544\n",
      "Epoch 24/100\n",
      "2687/2687 [==============================] - 1s 425us/step - loss: 0.0468 - val_loss: 0.0528\n",
      "Epoch 25/100\n",
      "2687/2687 [==============================] - 1s 437us/step - loss: 0.0453 - val_loss: 0.0513\n",
      "Epoch 26/100\n",
      "2687/2687 [==============================] - 1s 433us/step - loss: 0.0439 - val_loss: 0.0499\n",
      "Epoch 27/100\n",
      "2687/2687 [==============================] - 1s 433us/step - loss: 0.0425 - val_loss: 0.0486\n",
      "Epoch 28/100\n",
      "2687/2687 [==============================] - 1s 420us/step - loss: 0.0412 - val_loss: 0.0474\n",
      "Epoch 29/100\n",
      "2687/2687 [==============================] - 1s 446us/step - loss: 0.0401 - val_loss: 0.0462\n",
      "Epoch 30/100\n",
      "2687/2687 [==============================] - 1s 476us/step - loss: 0.0389 - val_loss: 0.0452\n",
      "Epoch 31/100\n",
      "2687/2687 [==============================] - 1s 507us/step - loss: 0.0379 - val_loss: 0.0442\n",
      "Epoch 32/100\n",
      "2687/2687 [==============================] - 1s 507us/step - loss: 0.0369 - val_loss: 0.0432\n",
      "Epoch 33/100\n",
      "2687/2687 [==============================] - 1s 490us/step - loss: 0.0360 - val_loss: 0.0423\n",
      "Epoch 34/100\n",
      "2687/2687 [==============================] - 1s 479us/step - loss: 0.0351 - val_loss: 0.0415\n",
      "Epoch 35/100\n",
      "2687/2687 [==============================] - 1s 468us/step - loss: 0.0342 - val_loss: 0.0407\n",
      "Epoch 36/100\n",
      "2687/2687 [==============================] - 1s 460us/step - loss: 0.0334 - val_loss: 0.0400\n",
      "Epoch 37/100\n",
      "2687/2687 [==============================] - 1s 458us/step - loss: 0.0327 - val_loss: 0.0392\n",
      "Epoch 38/100\n",
      "2687/2687 [==============================] - 1s 458us/step - loss: 0.0320 - val_loss: 0.0386\n",
      "Epoch 39/100\n",
      "2687/2687 [==============================] - 1s 465us/step - loss: 0.0313 - val_loss: 0.0379\n",
      "Epoch 40/100\n",
      "2687/2687 [==============================] - 1s 461us/step - loss: 0.0306 - val_loss: 0.0373\n",
      "Epoch 41/100\n",
      "2687/2687 [==============================] - 1s 469us/step - loss: 0.0300 - val_loss: 0.0367\n",
      "Epoch 42/100\n",
      "2687/2687 [==============================] - 1s 475us/step - loss: 0.0294 - val_loss: 0.0362\n",
      "Epoch 43/100\n",
      "2687/2687 [==============================] - 1s 493us/step - loss: 0.0288 - val_loss: 0.0357\n",
      "Epoch 44/100\n",
      "2687/2687 [==============================] - 1s 483us/step - loss: 0.0283 - val_loss: 0.0352\n",
      "Epoch 45/100\n",
      "2687/2687 [==============================] - 1s 495us/step - loss: 0.0278 - val_loss: 0.0347\n",
      "Epoch 46/100\n",
      "2687/2687 [==============================] - 1s 471us/step - loss: 0.0273 - val_loss: 0.0342\n",
      "Epoch 47/100\n",
      "2687/2687 [==============================] - 1s 475us/step - loss: 0.0268 - val_loss: 0.0338\n",
      "Epoch 48/100\n",
      "2687/2687 [==============================] - 1s 477us/step - loss: 0.0263 - val_loss: 0.0334\n",
      "Epoch 49/100\n",
      "2235/2687 [=======================>......] - ETA: 0s - loss: 0.0250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-4977d0006e00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(feats, target, batch_size=5, epochs=100, verbose=1, validation_split=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hidden layer with three nodes\n",
    "and a relu activation function and an output layer with one node and a sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(3, activation='relu', input_dim=2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2687 samples, validate on 672 samples\n",
      "Epoch 1/200\n",
      "2687/2687 [==============================] - 5s 2ms/step - loss: 0.5159 - val_loss: 0.3808\n",
      "Epoch 2/200\n",
      "2687/2687 [==============================] - 3s 948us/step - loss: 0.3258 - val_loss: 0.2992\n",
      "Epoch 3/200\n",
      "2687/2687 [==============================] - 2s 805us/step - loss: 0.2747 - val_loss: 0.2721\n",
      "Epoch 4/200\n",
      "2687/2687 [==============================] - 2s 799us/step - loss: 0.2518 - val_loss: 0.2555\n",
      "Epoch 5/200\n",
      "2687/2687 [==============================] - 2s 896us/step - loss: 0.2368 - val_loss: 0.2427\n",
      "Epoch 6/200\n",
      "2687/2687 [==============================] - 2s 782us/step - loss: 0.2256 - val_loss: 0.2327\n",
      "Epoch 7/200\n",
      "2687/2687 [==============================] - 2s 636us/step - loss: 0.2169 - val_loss: 0.2248\n",
      "Epoch 8/200\n",
      "2687/2687 [==============================] - 1s 481us/step - loss: 0.2100 - val_loss: 0.2177\n",
      "Epoch 9/200\n",
      "2687/2687 [==============================] - 1s 527us/step - loss: 0.2039 - val_loss: 0.2115\n",
      "Epoch 10/200\n",
      "2687/2687 [==============================] - 1s 499us/step - loss: 0.1985 - val_loss: 0.2062\n",
      "Epoch 11/200\n",
      "2687/2687 [==============================] - 1s 441us/step - loss: 0.1938 - val_loss: 0.2015\n",
      "Epoch 12/200\n",
      "2687/2687 [==============================] - 1s 503us/step - loss: 0.1896 - val_loss: 0.1976\n",
      "Epoch 13/200\n",
      "2687/2687 [==============================] - 1s 411us/step - loss: 0.1861 - val_loss: 0.1941\n",
      "Epoch 14/200\n",
      "2687/2687 [==============================] - 1s 450us/step - loss: 0.1829 - val_loss: 0.1911\n",
      "Epoch 15/200\n",
      "2687/2687 [==============================] - 1s 424us/step - loss: 0.1801 - val_loss: 0.1882\n",
      "Epoch 16/200\n",
      "2687/2687 [==============================] - 1s 448us/step - loss: 0.1776 - val_loss: 0.1856\n",
      "Epoch 17/200\n",
      "2687/2687 [==============================] - 1s 426us/step - loss: 0.1752 - val_loss: 0.1835\n",
      "Epoch 18/200\n",
      "2687/2687 [==============================] - 1s 444us/step - loss: 0.1732 - val_loss: 0.1812\n",
      "Epoch 19/200\n",
      "2687/2687 [==============================] - 1s 540us/step - loss: 0.1712 - val_loss: 0.1794\n",
      "Epoch 20/200\n",
      "2687/2687 [==============================] - 1s 454us/step - loss: 0.1695 - val_loss: 0.1774\n",
      "Epoch 21/200\n",
      "2687/2687 [==============================] - 1s 427us/step - loss: 0.1679 - val_loss: 0.1757\n",
      "Epoch 22/200\n",
      "2687/2687 [==============================] - 1s 508us/step - loss: 0.1664 - val_loss: 0.1743\n",
      "Epoch 23/200\n",
      "2687/2687 [==============================] - 1s 427us/step - loss: 0.1651 - val_loss: 0.1727\n",
      "Epoch 24/200\n",
      "2687/2687 [==============================] - 1s 439us/step - loss: 0.1639 - val_loss: 0.1714\n",
      "Epoch 25/200\n",
      "2687/2687 [==============================] - 1s 429us/step - loss: 0.1628 - val_loss: 0.1702\n",
      "Epoch 26/200\n",
      "2687/2687 [==============================] - 1s 499us/step - loss: 0.1617 - val_loss: 0.1691\n",
      "Epoch 27/200\n",
      "2687/2687 [==============================] - 1s 383us/step - loss: 0.1608 - val_loss: 0.1682\n",
      "Epoch 28/200\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.1600 - val_loss: 0.1670\n",
      "Epoch 29/200\n",
      "2687/2687 [==============================] - 1s 417us/step - loss: 0.1592 - val_loss: 0.1662\n",
      "Epoch 30/200\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.1584 - val_loss: 0.1654\n",
      "Epoch 31/200\n",
      "2687/2687 [==============================] - 1s 376us/step - loss: 0.1578 - val_loss: 0.1647\n",
      "Epoch 32/200\n",
      "2687/2687 [==============================] - 1s 459us/step - loss: 0.1571 - val_loss: 0.1640\n",
      "Epoch 33/200\n",
      "2687/2687 [==============================] - 1s 399us/step - loss: 0.1566 - val_loss: 0.1634\n",
      "Epoch 34/200\n",
      "2687/2687 [==============================] - 1s 488us/step - loss: 0.1560 - val_loss: 0.1628\n",
      "Epoch 35/200\n",
      "2687/2687 [==============================] - 1s 529us/step - loss: 0.1556 - val_loss: 0.1623\n",
      "Epoch 36/200\n",
      "2687/2687 [==============================] - 1s 424us/step - loss: 0.1551 - val_loss: 0.1618\n",
      "Epoch 37/200\n",
      "2687/2687 [==============================] - 1s 383us/step - loss: 0.1547 - val_loss: 0.1613\n",
      "Epoch 38/200\n",
      "2687/2687 [==============================] - 1s 396us/step - loss: 0.1543 - val_loss: 0.1609\n",
      "Epoch 39/200\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.1539 - val_loss: 0.1605\n",
      "Epoch 40/200\n",
      "2687/2687 [==============================] - 1s 387us/step - loss: 0.1536 - val_loss: 0.1601\n",
      "Epoch 41/200\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.1532 - val_loss: 0.1598\n",
      "Epoch 42/200\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.1529 - val_loss: 0.1595\n",
      "Epoch 43/200\n",
      "2687/2687 [==============================] - 1s 451us/step - loss: 0.1526 - val_loss: 0.1592\n",
      "Epoch 44/200\n",
      "2687/2687 [==============================] - 1s 438us/step - loss: 0.1524 - val_loss: 0.1589\n",
      "Epoch 45/200\n",
      "2687/2687 [==============================] - 1s 462us/step - loss: 0.1521 - val_loss: 0.1586\n",
      "Epoch 46/200\n",
      "2687/2687 [==============================] - 1s 400us/step - loss: 0.1519 - val_loss: 0.1584\n",
      "Epoch 47/200\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.1517 - val_loss: 0.1582\n",
      "Epoch 48/200\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.1515 - val_loss: 0.1580\n",
      "Epoch 49/200\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.1513 - val_loss: 0.1577\n",
      "Epoch 50/200\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.1511 - val_loss: 0.1575\n",
      "Epoch 51/200\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.1509 - val_loss: 0.1574\n",
      "Epoch 52/200\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.1507 - val_loss: 0.1572\n",
      "Epoch 53/200\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.1506 - val_loss: 0.1571\n",
      "Epoch 54/200\n",
      "2687/2687 [==============================] - 1s 415us/step - loss: 0.1504 - val_loss: 0.1569\n",
      "Epoch 55/200\n",
      "2687/2687 [==============================] - 1s 414us/step - loss: 0.1503 - val_loss: 0.1568\n",
      "Epoch 56/200\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.1501 - val_loss: 0.1566\n",
      "Epoch 57/200\n",
      "2687/2687 [==============================] - 1s 393us/step - loss: 0.1500 - val_loss: 0.1565\n",
      "Epoch 58/200\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.1499 - val_loss: 0.1564\n",
      "Epoch 59/200\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.1497 - val_loss: 0.1563\n",
      "Epoch 60/200\n",
      "2687/2687 [==============================] - 1s 546us/step - loss: 0.1496 - val_loss: 0.1561\n",
      "Epoch 61/200\n",
      "2687/2687 [==============================] - 1s 487us/step - loss: 0.1495 - val_loss: 0.1560\n",
      "Epoch 62/200\n",
      "2687/2687 [==============================] - 1s 499us/step - loss: 0.1494 - val_loss: 0.1559\n",
      "Epoch 63/200\n",
      "2687/2687 [==============================] - 1s 496us/step - loss: 0.1493 - val_loss: 0.1558\n",
      "Epoch 64/200\n",
      "2687/2687 [==============================] - 1s 427us/step - loss: 0.1492 - val_loss: 0.1557\n",
      "Epoch 65/200\n",
      "2687/2687 [==============================] - 1s 450us/step - loss: 0.1491 - val_loss: 0.1556\n",
      "Epoch 66/200\n",
      "2687/2687 [==============================] - 1s 441us/step - loss: 0.1490 - val_loss: 0.1555\n",
      "Epoch 67/200\n",
      "2687/2687 [==============================] - 1s 444us/step - loss: 0.1489 - val_loss: 0.1554\n",
      "Epoch 68/200\n",
      "2687/2687 [==============================] - 1s 482us/step - loss: 0.1489 - val_loss: 0.1554\n",
      "Epoch 69/200\n",
      "2687/2687 [==============================] - 1s 475us/step - loss: 0.1488 - val_loss: 0.1553\n",
      "Epoch 70/200\n",
      "2687/2687 [==============================] - 1s 438us/step - loss: 0.1487 - val_loss: 0.1552\n",
      "Epoch 71/200\n",
      "2687/2687 [==============================] - 1s 421us/step - loss: 0.1486 - val_loss: 0.1551\n",
      "Epoch 72/200\n",
      "2687/2687 [==============================] - 1s 470us/step - loss: 0.1486 - val_loss: 0.1550\n",
      "Epoch 73/200\n",
      "2687/2687 [==============================] - 1s 427us/step - loss: 0.1485 - val_loss: 0.1550\n",
      "Epoch 74/200\n",
      "2687/2687 [==============================] - 1s 478us/step - loss: 0.1484 - val_loss: 0.1549\n",
      "Epoch 75/200\n",
      "2687/2687 [==============================] - 1s 557us/step - loss: 0.1484 - val_loss: 0.1548\n",
      "Epoch 76/200\n",
      "2687/2687 [==============================] - 1s 457us/step - loss: 0.1483 - val_loss: 0.1548\n",
      "Epoch 77/200\n",
      "2687/2687 [==============================] - 1s 429us/step - loss: 0.1482 - val_loss: 0.1547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "2687/2687 [==============================] - 1s 392us/step - loss: 0.1482 - val_loss: 0.1546\n",
      "Epoch 79/200\n",
      "2687/2687 [==============================] - 1s 397us/step - loss: 0.1481 - val_loss: 0.1546\n",
      "Epoch 80/200\n",
      "2687/2687 [==============================] - 1s 399us/step - loss: 0.1481 - val_loss: 0.1545\n",
      "Epoch 81/200\n",
      "2687/2687 [==============================] - 1s 389us/step - loss: 0.1480 - val_loss: 0.1545\n",
      "Epoch 82/200\n",
      "2687/2687 [==============================] - 1s 387us/step - loss: 0.1480 - val_loss: 0.1544\n",
      "Epoch 83/200\n",
      "2687/2687 [==============================] - 1s 418us/step - loss: 0.1479 - val_loss: 0.1544\n",
      "Epoch 84/200\n",
      "2687/2687 [==============================] - 1s 432us/step - loss: 0.1479 - val_loss: 0.1543\n",
      "Epoch 85/200\n",
      "2687/2687 [==============================] - 1s 434us/step - loss: 0.1478 - val_loss: 0.1543\n",
      "Epoch 86/200\n",
      "2687/2687 [==============================] - 1s 429us/step - loss: 0.1478 - val_loss: 0.1542\n",
      "Epoch 87/200\n",
      "2687/2687 [==============================] - 1s 426us/step - loss: 0.1477 - val_loss: 0.1542\n",
      "Epoch 88/200\n",
      "2687/2687 [==============================] - 1s 442us/step - loss: 0.1477 - val_loss: 0.1541\n",
      "Epoch 89/200\n",
      "2687/2687 [==============================] - 1s 445us/step - loss: 0.1476 - val_loss: 0.1541\n",
      "Epoch 90/200\n",
      "2687/2687 [==============================] - 1s 430us/step - loss: 0.1476 - val_loss: 0.1541\n",
      "Epoch 91/200\n",
      "2687/2687 [==============================] - 1s 490us/step - loss: 0.1476 - val_loss: 0.1540\n",
      "Epoch 92/200\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.1475 - val_loss: 0.1540\n",
      "Epoch 93/200\n",
      "2687/2687 [==============================] - 1s 406us/step - loss: 0.1475 - val_loss: 0.1539\n",
      "Epoch 94/200\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.1475 - val_loss: 0.1539\n",
      "Epoch 95/200\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.1474 - val_loss: 0.1539\n",
      "Epoch 96/200\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.1474 - val_loss: 0.1539\n",
      "Epoch 97/200\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.1473 - val_loss: 0.1538\n",
      "Epoch 98/200\n",
      "2687/2687 [==============================] - 1s 378us/step - loss: 0.1473 - val_loss: 0.1538\n",
      "Epoch 99/200\n",
      "2687/2687 [==============================] - 1s 387us/step - loss: 0.1473 - val_loss: 0.1538\n",
      "Epoch 100/200\n",
      "2687/2687 [==============================] - 1s 393us/step - loss: 0.1473 - val_loss: 0.1537\n",
      "Epoch 101/200\n",
      "2687/2687 [==============================] - 1s 393us/step - loss: 0.1472 - val_loss: 0.1537\n",
      "Epoch 102/200\n",
      "2687/2687 [==============================] - 1s 397us/step - loss: 0.1472 - val_loss: 0.1537\n",
      "Epoch 103/200\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.1472 - val_loss: 0.1537\n",
      "Epoch 104/200\n",
      "2687/2687 [==============================] - 1s 385us/step - loss: 0.1471 - val_loss: 0.1536\n",
      "Epoch 105/200\n",
      "2687/2687 [==============================] - 1s 384us/step - loss: 0.1471 - val_loss: 0.1536\n",
      "Epoch 106/200\n",
      "2687/2687 [==============================] - 1s 378us/step - loss: 0.1471 - val_loss: 0.1536\n",
      "Epoch 107/200\n",
      "2687/2687 [==============================] - 1s 405us/step - loss: 0.1471 - val_loss: 0.1535\n",
      "Epoch 108/200\n",
      "2687/2687 [==============================] - 1s 392us/step - loss: 0.1470 - val_loss: 0.1535\n",
      "Epoch 109/200\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.1470 - val_loss: 0.1534\n",
      "Epoch 110/200\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.1470 - val_loss: 0.1534\n",
      "Epoch 111/200\n",
      "2687/2687 [==============================] - 1s 409us/step - loss: 0.1470 - val_loss: 0.1534\n",
      "Epoch 112/200\n",
      "2687/2687 [==============================] - 1s 375us/step - loss: 0.1469 - val_loss: 0.1534\n",
      "Epoch 113/200\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.1469 - val_loss: 0.1533\n",
      "Epoch 114/200\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.1469 - val_loss: 0.1533\n",
      "Epoch 115/200\n",
      "2687/2687 [==============================] - 1s 439us/step - loss: 0.1469 - val_loss: 0.1533\n",
      "Epoch 116/200\n",
      "2687/2687 [==============================] - 1s 469us/step - loss: 0.1468 - val_loss: 0.1533\n",
      "Epoch 117/200\n",
      "2687/2687 [==============================] - 1s 469us/step - loss: 0.1468 - val_loss: 0.1532\n",
      "Epoch 118/200\n",
      "2687/2687 [==============================] - 1s 426us/step - loss: 0.1468 - val_loss: 0.1532\n",
      "Epoch 119/200\n",
      "2687/2687 [==============================] - 1s 421us/step - loss: 0.1468 - val_loss: 0.1532\n",
      "Epoch 120/200\n",
      "2687/2687 [==============================] - 1s 424us/step - loss: 0.1468 - val_loss: 0.1532\n",
      "Epoch 121/200\n",
      "2687/2687 [==============================] - 1s 433us/step - loss: 0.1468 - val_loss: 0.1532\n",
      "Epoch 122/200\n",
      "2687/2687 [==============================] - 1s 438us/step - loss: 0.1467 - val_loss: 0.1532\n",
      "Epoch 123/200\n",
      "2687/2687 [==============================] - 1s 448us/step - loss: 0.1467 - val_loss: 0.1531\n",
      "Epoch 124/200\n",
      "2687/2687 [==============================] - 1s 423us/step - loss: 0.1467 - val_loss: 0.1531\n",
      "Epoch 125/200\n",
      "2687/2687 [==============================] - 1s 441us/step - loss: 0.1467 - val_loss: 0.1531\n",
      "Epoch 126/200\n",
      "2687/2687 [==============================] - 1s 450us/step - loss: 0.1467 - val_loss: 0.1531\n",
      "Epoch 127/200\n",
      "2687/2687 [==============================] - 1s 438us/step - loss: 0.1467 - val_loss: 0.1531\n",
      "Epoch 128/200\n",
      "2687/2687 [==============================] - 1s 472us/step - loss: 0.1466 - val_loss: 0.1530\n",
      "Epoch 129/200\n",
      "2687/2687 [==============================] - 1s 509us/step - loss: 0.1466 - val_loss: 0.1530\n",
      "Epoch 130/200\n",
      "2687/2687 [==============================] - 1s 375us/step - loss: 0.1466 - val_loss: 0.1530\n",
      "Epoch 131/200\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.1466 - val_loss: 0.1530\n",
      "Epoch 132/200\n",
      "2687/2687 [==============================] - 1s 445us/step - loss: 0.1466 - val_loss: 0.1530\n",
      "Epoch 133/200\n",
      "2687/2687 [==============================] - 1s 390us/step - loss: 0.1466 - val_loss: 0.1530\n",
      "Epoch 134/200\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.1466 - val_loss: 0.1529\n",
      "Epoch 135/200\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.1465 - val_loss: 0.1529\n",
      "Epoch 136/200\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.1465 - val_loss: 0.1529\n",
      "Epoch 137/200\n",
      "2687/2687 [==============================] - 1s 420us/step - loss: 0.1465 - val_loss: 0.1529\n",
      "Epoch 138/200\n",
      "2687/2687 [==============================] - 1s 499us/step - loss: 0.1465 - val_loss: 0.1529\n",
      "Epoch 139/200\n",
      "2687/2687 [==============================] - 1s 421us/step - loss: 0.1465 - val_loss: 0.1529\n",
      "Epoch 140/200\n",
      "2687/2687 [==============================] - 1s 438us/step - loss: 0.1465 - val_loss: 0.1529\n",
      "Epoch 141/200\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.143 - 1s 435us/step - loss: 0.1465 - val_loss: 0.1528\n",
      "Epoch 142/200\n",
      "2687/2687 [==============================] - 1s 435us/step - loss: 0.1465 - val_loss: 0.1528\n",
      "Epoch 143/200\n",
      "2687/2687 [==============================] - 1s 432us/step - loss: 0.1465 - val_loss: 0.1528\n",
      "Epoch 144/200\n",
      "2687/2687 [==============================] - 1s 517us/step - loss: 0.1464 - val_loss: 0.1528\n",
      "Epoch 145/200\n",
      "2687/2687 [==============================] - 1s 406us/step - loss: 0.1464 - val_loss: 0.1528\n",
      "Epoch 146/200\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.1464 - val_loss: 0.1528\n",
      "Epoch 147/200\n",
      "2687/2687 [==============================] - 1s 383us/step - loss: 0.1464 - val_loss: 0.1528\n",
      "Epoch 148/200\n",
      "2687/2687 [==============================] - 1s 399us/step - loss: 0.1464 - val_loss: 0.1528\n",
      "Epoch 149/200\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.1464 - val_loss: 0.1527\n",
      "Epoch 150/200\n",
      "2687/2687 [==============================] - 1s 469us/step - loss: 0.1464 - val_loss: 0.1527\n",
      "Epoch 151/200\n",
      "2687/2687 [==============================] - 1s 467us/step - loss: 0.1464 - val_loss: 0.1527\n",
      "Epoch 152/200\n",
      "2687/2687 [==============================] - 1s 453us/step - loss: 0.1464 - val_loss: 0.1527\n",
      "Epoch 153/200\n",
      "2687/2687 [==============================] - 1s 442us/step - loss: 0.1463 - val_loss: 0.1527\n",
      "Epoch 154/200\n",
      "2687/2687 [==============================] - 1s 418us/step - loss: 0.1463 - val_loss: 0.1527\n",
      "Epoch 155/200\n",
      "2687/2687 [==============================] - 1s 405us/step - loss: 0.1463 - val_loss: 0.1527\n",
      "Epoch 156/200\n",
      "2687/2687 [==============================] - 1s 494us/step - loss: 0.1463 - val_loss: 0.1527\n",
      "Epoch 157/200\n",
      "2687/2687 [==============================] - 1s 396us/step - loss: 0.1463 - val_loss: 0.1526\n",
      "Epoch 158/200\n",
      "2687/2687 [==============================] - 1s 431us/step - loss: 0.1463 - val_loss: 0.1526\n",
      "Epoch 159/200\n",
      "2687/2687 [==============================] - 1s 381us/step - loss: 0.1463 - val_loss: 0.1526\n",
      "Epoch 160/200\n",
      "2687/2687 [==============================] - 1s 447us/step - loss: 0.1463 - val_loss: 0.1526\n",
      "Epoch 161/200\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.1463 - val_loss: 0.1526\n",
      "Epoch 162/200\n",
      "2687/2687 [==============================] - 1s 377us/step - loss: 0.1463 - val_loss: 0.1526\n",
      "Epoch 163/200\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.1462 - val_loss: 0.1526\n",
      "Epoch 164/200\n",
      "2687/2687 [==============================] - 1s 408us/step - loss: 0.1462 - val_loss: 0.1526\n",
      "Epoch 165/200\n",
      "2687/2687 [==============================] - 1s 536us/step - loss: 0.1462 - val_loss: 0.1526\n",
      "Epoch 166/200\n",
      "2687/2687 [==============================] - 1s 494us/step - loss: 0.1462 - val_loss: 0.1526\n",
      "Epoch 167/200\n",
      "2687/2687 [==============================] - 1s 478us/step - loss: 0.1462 - val_loss: 0.1526\n",
      "Epoch 168/200\n",
      "2687/2687 [==============================] - 1s 460us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 169/200\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 170/200\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 171/200\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 172/200\n",
      "2687/2687 [==============================] - 1s 439us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 173/200\n",
      "2687/2687 [==============================] - 1s 417us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 174/200\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 175/200\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.1462 - val_loss: 0.1525\n",
      "Epoch 176/200\n",
      "2687/2687 [==============================] - 1s 373us/step - loss: 0.1461 - val_loss: 0.1525\n",
      "Epoch 177/200\n",
      "2687/2687 [==============================] - 1s 384us/step - loss: 0.1461 - val_loss: 0.1525\n",
      "Epoch 178/200\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.1461 - val_loss: 0.1525\n",
      "Epoch 179/200\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 180/200\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 181/200\n",
      "2687/2687 [==============================] - 1s 430us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 182/200\n",
      "2687/2687 [==============================] - 1s 436us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 183/200\n",
      "2687/2687 [==============================] - 1s 396us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 184/200\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 185/200\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 186/200\n",
      "2687/2687 [==============================] - 1s 395us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 187/200\n",
      "2687/2687 [==============================] - 1s 488us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 188/200\n",
      "2687/2687 [==============================] - 1s 442us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 189/200\n",
      "2687/2687 [==============================] - 1s 408us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 190/200\n",
      "2687/2687 [==============================] - 1s 430us/step - loss: 0.1461 - val_loss: 0.1524\n",
      "Epoch 191/200\n",
      "2687/2687 [==============================] - 1s 436us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 192/200\n",
      "2687/2687 [==============================] - 1s 415us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 193/200\n",
      "2687/2687 [==============================] - 1s 426us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 194/200\n",
      "2687/2687 [==============================] - 1s 423us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 195/200\n",
      "2687/2687 [==============================] - 1s 489us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 196/200\n",
      "2687/2687 [==============================] - 1s 445us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 197/200\n",
      "2687/2687 [==============================] - 1s 459us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 198/200\n",
      "2687/2687 [==============================] - 1s 456us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 199/200\n",
      "2687/2687 [==============================] - 1s 418us/step - loss: 0.1460 - val_loss: 0.1523\n",
      "Epoch 200/200\n",
      "2687/2687 [==============================] - 1s 405us/step - loss: 0.1460 - val_loss: 0.1523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29cbe35438>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(feats, target, batch_size=5, epochs=200, verbose=1, validation_split=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hidden layer with six nodes and a relu activation function and an output layer with one node and a sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(6, activation='relu', input_dim=2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2687 samples, validate on 672 samples\n",
      "Epoch 1/400\n",
      "2687/2687 [==============================] - 4s 2ms/step - loss: 0.4545 - val_loss: 0.3447\n",
      "Epoch 2/400\n",
      "2687/2687 [==============================] - 2s 676us/step - loss: 0.2942 - val_loss: 0.2772\n",
      "Epoch 3/400\n",
      "2687/2687 [==============================] - 2s 667us/step - loss: 0.2397 - val_loss: 0.2357\n",
      "Epoch 4/400\n",
      "2687/2687 [==============================] - 2s 634us/step - loss: 0.2008 - val_loss: 0.2004\n",
      "Epoch 5/400\n",
      "2687/2687 [==============================] - 2s 588us/step - loss: 0.1686 - val_loss: 0.1680\n",
      "Epoch 6/400\n",
      "2687/2687 [==============================] - 1s 491us/step - loss: 0.1424 - val_loss: 0.1419\n",
      "Epoch 7/400\n",
      "2687/2687 [==============================] - 1s 432us/step - loss: 0.1215 - val_loss: 0.1210\n",
      "Epoch 8/400\n",
      "2687/2687 [==============================] - 1s 392us/step - loss: 0.1047 - val_loss: 0.1051\n",
      "Epoch 9/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0917 - val_loss: 0.0927\n",
      "Epoch 10/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0815 - val_loss: 0.0832\n",
      "Epoch 11/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0734 - val_loss: 0.0754\n",
      "Epoch 12/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0669 - val_loss: 0.0693\n",
      "Epoch 13/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0615 - val_loss: 0.0644\n",
      "Epoch 14/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0569 - val_loss: 0.0603\n",
      "Epoch 15/400\n",
      "2687/2687 [==============================] - 1s 348us/step - loss: 0.0530 - val_loss: 0.0568\n",
      "Epoch 16/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0498 - val_loss: 0.0540\n",
      "Epoch 17/400\n",
      "2687/2687 [==============================] - 1s 349us/step - loss: 0.0470 - val_loss: 0.0516\n",
      "Epoch 18/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0446 - val_loss: 0.0495\n",
      "Epoch 19/400\n",
      "2687/2687 [==============================] - 1s 375us/step - loss: 0.0424 - val_loss: 0.0477\n",
      "Epoch 20/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0406 - val_loss: 0.0461\n",
      "Epoch 21/400\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.0389 - val_loss: 0.0447\n",
      "Epoch 22/400\n",
      "2687/2687 [==============================] - 1s 420us/step - loss: 0.0374 - val_loss: 0.0434\n",
      "Epoch 23/400\n",
      "2687/2687 [==============================] - 1s 435us/step - loss: 0.0361 - val_loss: 0.0423\n",
      "Epoch 24/400\n",
      "2687/2687 [==============================] - 1s 454us/step - loss: 0.0349 - val_loss: 0.0412\n",
      "Epoch 25/400\n",
      "2687/2687 [==============================] - 1s 415us/step - loss: 0.0338 - val_loss: 0.0403\n",
      "Epoch 26/400\n",
      "2687/2687 [==============================] - 2s 596us/step - loss: 0.0328 - val_loss: 0.0394\n",
      "Epoch 27/400\n",
      "2687/2687 [==============================] - 1s 491us/step - loss: 0.0319 - val_loss: 0.0386\n",
      "Epoch 28/400\n",
      "2687/2687 [==============================] - 1s 375us/step - loss: 0.0310 - val_loss: 0.0379\n",
      "Epoch 29/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0303 - val_loss: 0.0372\n",
      "Epoch 30/400\n",
      "2687/2687 [==============================] - 1s 390us/step - loss: 0.0295 - val_loss: 0.0366\n",
      "Epoch 31/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0289 - val_loss: 0.0360\n",
      "Epoch 32/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0283 - val_loss: 0.0355\n",
      "Epoch 33/400\n",
      "2687/2687 [==============================] - 1s 345us/step - loss: 0.0277 - val_loss: 0.0350\n",
      "Epoch 34/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0271 - val_loss: 0.0345\n",
      "Epoch 35/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0266 - val_loss: 0.0341\n",
      "Epoch 36/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0261 - val_loss: 0.0337\n",
      "Epoch 37/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0257 - val_loss: 0.0333\n",
      "Epoch 38/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0253 - val_loss: 0.0329\n",
      "Epoch 39/400\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.0249 - val_loss: 0.0326\n",
      "Epoch 40/400\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.0245 - val_loss: 0.0323\n",
      "Epoch 41/400\n",
      "2687/2687 [==============================] - 1s 411us/step - loss: 0.0241 - val_loss: 0.0320\n",
      "Epoch 42/400\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.0238 - val_loss: 0.0317\n",
      "Epoch 43/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0234 - val_loss: 0.0314\n",
      "Epoch 44/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0231 - val_loss: 0.0312\n",
      "Epoch 45/400\n",
      "2687/2687 [==============================] - 1s 377us/step - loss: 0.0227 - val_loss: 0.0310\n",
      "Epoch 46/400\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.0224 - val_loss: 0.0308\n",
      "Epoch 47/400\n",
      "2687/2687 [==============================] - 1s 377us/step - loss: 0.0221 - val_loss: 0.0306\n",
      "Epoch 48/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0218 - val_loss: 0.0304\n",
      "Epoch 49/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0215 - val_loss: 0.0303\n",
      "Epoch 50/400\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.0213 - val_loss: 0.0301\n",
      "Epoch 51/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0210 - val_loss: 0.0300\n",
      "Epoch 52/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0208 - val_loss: 0.0298\n",
      "Epoch 53/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0205 - val_loss: 0.0297\n",
      "Epoch 54/400\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.020 - 1s 378us/step - loss: 0.0203 - val_loss: 0.0296\n",
      "Epoch 55/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0201 - val_loss: 0.0295\n",
      "Epoch 56/400\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.0199 - val_loss: 0.0293\n",
      "Epoch 57/400\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.0197 - val_loss: 0.0292\n",
      "Epoch 58/400\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.0195 - val_loss: 0.0291\n",
      "Epoch 59/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0193 - val_loss: 0.0290\n",
      "Epoch 60/400\n",
      "2687/2687 [==============================] - 1s 384us/step - loss: 0.0191 - val_loss: 0.0289\n",
      "Epoch 61/400\n",
      "2687/2687 [==============================] - 1s 429us/step - loss: 0.0189 - val_loss: 0.0288\n",
      "Epoch 62/400\n",
      "2687/2687 [==============================] - 1s 375us/step - loss: 0.0187 - val_loss: 0.0288\n",
      "Epoch 63/400\n",
      "2687/2687 [==============================] - 1s 389us/step - loss: 0.0186 - val_loss: 0.0287\n",
      "Epoch 64/400\n",
      "2687/2687 [==============================] - 1s 426us/step - loss: 0.0184 - val_loss: 0.0286\n",
      "Epoch 65/400\n",
      "2687/2687 [==============================] - 1s 490us/step - loss: 0.0182 - val_loss: 0.0285\n",
      "Epoch 66/400\n",
      "2687/2687 [==============================] - 1s 454us/step - loss: 0.0181 - val_loss: 0.0285\n",
      "Epoch 67/400\n",
      "2687/2687 [==============================] - 1s 493us/step - loss: 0.0179 - val_loss: 0.0284\n",
      "Epoch 68/400\n",
      "2687/2687 [==============================] - 1s 445us/step - loss: 0.0178 - val_loss: 0.0283\n",
      "Epoch 69/400\n",
      "2687/2687 [==============================] - 1s 463us/step - loss: 0.0177 - val_loss: 0.0283\n",
      "Epoch 70/400\n",
      "2687/2687 [==============================] - 1s 447us/step - loss: 0.0175 - val_loss: 0.0282\n",
      "Epoch 71/400\n",
      "2687/2687 [==============================] - 1s 450us/step - loss: 0.0174 - val_loss: 0.0282\n",
      "Epoch 72/400\n",
      "2687/2687 [==============================] - 1s 496us/step - loss: 0.0173 - val_loss: 0.0281\n",
      "Epoch 73/400\n",
      "2687/2687 [==============================] - 1s 414us/step - loss: 0.0171 - val_loss: 0.0281\n",
      "Epoch 74/400\n",
      "2687/2687 [==============================] - 1s 409us/step - loss: 0.0170 - val_loss: 0.0280\n",
      "Epoch 75/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0169 - val_loss: 0.0279\n",
      "Epoch 76/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0168 - val_loss: 0.0279\n",
      "Epoch 77/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0167 - val_loss: 0.0278\n",
      "Epoch 78/400\n",
      "2687/2687 [==============================] - 1s 383us/step - loss: 0.0166 - val_loss: 0.0277\n",
      "Epoch 79/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0165 - val_loss: 0.0277\n",
      "Epoch 80/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0164 - val_loss: 0.0276\n",
      "Epoch 81/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0163 - val_loss: 0.0275\n",
      "Epoch 82/400\n",
      "2687/2687 [==============================] - 1s 378us/step - loss: 0.0162 - val_loss: 0.0275\n",
      "Epoch 83/400\n",
      "2687/2687 [==============================] - 1s 462us/step - loss: 0.0161 - val_loss: 0.0274\n",
      "Epoch 84/400\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.0160 - val_loss: 0.0274\n",
      "Epoch 85/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0159 - val_loss: 0.0273\n",
      "Epoch 86/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0158 - val_loss: 0.0272\n",
      "Epoch 87/400\n",
      "2687/2687 [==============================] - 1s 411us/step - loss: 0.0157 - val_loss: 0.0271\n",
      "Epoch 88/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0156 - val_loss: 0.0270\n",
      "Epoch 89/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0155 - val_loss: 0.0269\n",
      "Epoch 90/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0154 - val_loss: 0.0268\n",
      "Epoch 91/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0154 - val_loss: 0.0268\n",
      "Epoch 92/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0153 - val_loss: 0.0266\n",
      "Epoch 93/400\n",
      "2687/2687 [==============================] - 1s 384us/step - loss: 0.0152 - val_loss: 0.0265\n",
      "Epoch 94/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0151 - val_loss: 0.0264\n",
      "Epoch 95/400\n",
      "2687/2687 [==============================] - 1s 426us/step - loss: 0.0150 - val_loss: 0.0263\n",
      "Epoch 96/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0149 - val_loss: 0.0262\n",
      "Epoch 97/400\n",
      "2687/2687 [==============================] - 1s 377us/step - loss: 0.0148 - val_loss: 0.0261\n",
      "Epoch 98/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0147 - val_loss: 0.0260\n",
      "Epoch 99/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0146 - val_loss: 0.0259\n",
      "Epoch 100/400\n",
      "2687/2687 [==============================] - 1s 389us/step - loss: 0.0145 - val_loss: 0.0258\n",
      "Epoch 101/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0144 - val_loss: 0.0257\n",
      "Epoch 102/400\n",
      "2687/2687 [==============================] - 1s 403us/step - loss: 0.0144 - val_loss: 0.0256\n",
      "Epoch 103/400\n",
      "2687/2687 [==============================] - 1s 386us/step - loss: 0.0143 - val_loss: 0.0256\n",
      "Epoch 104/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0142 - val_loss: 0.0255\n",
      "Epoch 105/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0141 - val_loss: 0.0254\n",
      "Epoch 106/400\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.0140 - val_loss: 0.0254\n",
      "Epoch 107/400\n",
      "2687/2687 [==============================] - 1s 377us/step - loss: 0.0140 - val_loss: 0.0253\n",
      "Epoch 108/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0139 - val_loss: 0.0253\n",
      "Epoch 109/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0138 - val_loss: 0.0252\n",
      "Epoch 110/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0137 - val_loss: 0.0251\n",
      "Epoch 111/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0137 - val_loss: 0.0251\n",
      "Epoch 112/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0136 - val_loss: 0.0250\n",
      "Epoch 113/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0135 - val_loss: 0.0250\n",
      "Epoch 114/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0135 - val_loss: 0.0249\n",
      "Epoch 115/400\n",
      "2687/2687 [==============================] - 1s 409us/step - loss: 0.0134 - val_loss: 0.0249\n",
      "Epoch 116/400\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.0134 - val_loss: 0.0249\n",
      "Epoch 117/400\n",
      "2687/2687 [==============================] - 1s 396us/step - loss: 0.0133 - val_loss: 0.0248\n",
      "Epoch 118/400\n",
      "2687/2687 [==============================] - 1s 383us/step - loss: 0.0132 - val_loss: 0.0248\n",
      "Epoch 119/400\n",
      "2687/2687 [==============================] - 1s 395us/step - loss: 0.0132 - val_loss: 0.0247\n",
      "Epoch 120/400\n",
      "2687/2687 [==============================] - 1s 409us/step - loss: 0.0131 - val_loss: 0.0247\n",
      "Epoch 121/400\n",
      "2687/2687 [==============================] - 1s 399us/step - loss: 0.0131 - val_loss: 0.0246\n",
      "Epoch 122/400\n",
      "2687/2687 [==============================] - 1s 378us/step - loss: 0.0130 - val_loss: 0.0246\n",
      "Epoch 123/400\n",
      "2687/2687 [==============================] - 1s 380us/step - loss: 0.0129 - val_loss: 0.0246\n",
      "Epoch 124/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0129 - val_loss: 0.0245\n",
      "Epoch 125/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0128 - val_loss: 0.0245\n",
      "Epoch 126/400\n",
      "2687/2687 [==============================] - 1s 414us/step - loss: 0.0128 - val_loss: 0.0245\n",
      "Epoch 127/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0127 - val_loss: 0.0244\n",
      "Epoch 128/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0127 - val_loss: 0.0244\n",
      "Epoch 129/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0126 - val_loss: 0.0243\n",
      "Epoch 130/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0126 - val_loss: 0.0243\n",
      "Epoch 131/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0125 - val_loss: 0.0243\n",
      "Epoch 132/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0125 - val_loss: 0.0242\n",
      "Epoch 133/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0124 - val_loss: 0.0242\n",
      "Epoch 134/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0124 - val_loss: 0.0242\n",
      "Epoch 135/400\n",
      "2687/2687 [==============================] - 1s 361us/step - loss: 0.0123 - val_loss: 0.0241\n",
      "Epoch 136/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0123 - val_loss: 0.0241\n",
      "Epoch 137/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0123 - val_loss: 0.0241\n",
      "Epoch 138/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0122 - val_loss: 0.0241\n",
      "Epoch 139/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0122 - val_loss: 0.0240\n",
      "Epoch 140/400\n",
      "2687/2687 [==============================] - 1s 348us/step - loss: 0.0121 - val_loss: 0.0239\n",
      "Epoch 141/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0121 - val_loss: 0.0239\n",
      "Epoch 142/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0120 - val_loss: 0.0239\n",
      "Epoch 143/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0120 - val_loss: 0.0239\n",
      "Epoch 144/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0119 - val_loss: 0.0238\n",
      "Epoch 145/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0119 - val_loss: 0.0238\n",
      "Epoch 146/400\n",
      "2687/2687 [==============================] - 1s 441us/step - loss: 0.0119 - val_loss: 0.0238\n",
      "Epoch 147/400\n",
      "2687/2687 [==============================] - 1s 536us/step - loss: 0.0118 - val_loss: 0.0238\n",
      "Epoch 148/400\n",
      "2687/2687 [==============================] - 1s 499us/step - loss: 0.0118 - val_loss: 0.0237\n",
      "Epoch 149/400\n",
      "2687/2687 [==============================] - 1s 396us/step - loss: 0.0118 - val_loss: 0.0237\n",
      "Epoch 150/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0117 - val_loss: 0.0237\n",
      "Epoch 151/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0117 - val_loss: 0.0237\n",
      "Epoch 152/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0116 - val_loss: 0.0236\n",
      "Epoch 153/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0116 - val_loss: 0.0236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0116 - val_loss: 0.0236\n",
      "Epoch 155/400\n",
      "2687/2687 [==============================] - 1s 364us/step - loss: 0.0115 - val_loss: 0.0236\n",
      "Epoch 156/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0115 - val_loss: 0.0235\n",
      "Epoch 157/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0115 - val_loss: 0.0235\n",
      "Epoch 158/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0114 - val_loss: 0.0235\n",
      "Epoch 159/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0114 - val_loss: 0.0235\n",
      "Epoch 160/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0114 - val_loss: 0.0234\n",
      "Epoch 161/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0113 - val_loss: 0.0234\n",
      "Epoch 162/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0113 - val_loss: 0.0234\n",
      "Epoch 163/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0113 - val_loss: 0.0234\n",
      "Epoch 164/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0112 - val_loss: 0.0233\n",
      "Epoch 165/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0112 - val_loss: 0.0233\n",
      "Epoch 166/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0112 - val_loss: 0.0233\n",
      "Epoch 167/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0111 - val_loss: 0.0233\n",
      "Epoch 168/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0111 - val_loss: 0.0233\n",
      "Epoch 169/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0111 - val_loss: 0.0232\n",
      "Epoch 170/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0110 - val_loss: 0.0232\n",
      "Epoch 171/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0110 - val_loss: 0.0232\n",
      "Epoch 172/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0110 - val_loss: 0.0231\n",
      "Epoch 173/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0109 - val_loss: 0.0231\n",
      "Epoch 174/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0109 - val_loss: 0.0231\n",
      "Epoch 175/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0109 - val_loss: 0.0230\n",
      "Epoch 176/400\n",
      "2687/2687 [==============================] - 1s 352us/step - loss: 0.0108 - val_loss: 0.0230\n",
      "Epoch 177/400\n",
      "2687/2687 [==============================] - 1s 377us/step - loss: 0.0108 - val_loss: 0.0230\n",
      "Epoch 178/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0108 - val_loss: 0.0230\n",
      "Epoch 179/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0107 - val_loss: 0.0230\n",
      "Epoch 180/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0107 - val_loss: 0.0229\n",
      "Epoch 181/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0107 - val_loss: 0.0229\n",
      "Epoch 182/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0106 - val_loss: 0.0229\n",
      "Epoch 183/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0106 - val_loss: 0.0229\n",
      "Epoch 184/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0106 - val_loss: 0.0229\n",
      "Epoch 185/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0105 - val_loss: 0.0229\n",
      "Epoch 186/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0105 - val_loss: 0.0229\n",
      "Epoch 187/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0105 - val_loss: 0.0229\n",
      "Epoch 188/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0105 - val_loss: 0.0229\n",
      "Epoch 189/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0104 - val_loss: 0.0229\n",
      "Epoch 190/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0104 - val_loss: 0.0229\n",
      "Epoch 191/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0104 - val_loss: 0.0229\n",
      "Epoch 192/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0103 - val_loss: 0.0229\n",
      "Epoch 193/400\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.0103 - val_loss: 0.0229\n",
      "Epoch 194/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0103 - val_loss: 0.0229\n",
      "Epoch 195/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0103 - val_loss: 0.0229\n",
      "Epoch 196/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0102 - val_loss: 0.0229\n",
      "Epoch 197/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0102 - val_loss: 0.0229\n",
      "Epoch 198/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0102 - val_loss: 0.0229\n",
      "Epoch 199/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0102 - val_loss: 0.0229\n",
      "Epoch 200/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0101 - val_loss: 0.0229\n",
      "Epoch 201/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0101 - val_loss: 0.0229\n",
      "Epoch 202/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0101 - val_loss: 0.0228\n",
      "Epoch 203/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0101 - val_loss: 0.0228\n",
      "Epoch 204/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0100 - val_loss: 0.0228\n",
      "Epoch 205/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0100 - val_loss: 0.0228\n",
      "Epoch 206/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0100 - val_loss: 0.0228\n",
      "Epoch 207/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0100 - val_loss: 0.0228\n",
      "Epoch 208/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0100 - val_loss: 0.0228\n",
      "Epoch 209/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0099 - val_loss: 0.0228\n",
      "Epoch 210/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0099 - val_loss: 0.0228\n",
      "Epoch 211/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0099 - val_loss: 0.0228\n",
      "Epoch 212/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0099 - val_loss: 0.0228\n",
      "Epoch 213/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0098 - val_loss: 0.0228\n",
      "Epoch 214/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0098 - val_loss: 0.0228\n",
      "Epoch 215/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0098 - val_loss: 0.0228\n",
      "Epoch 216/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0098 - val_loss: 0.0229\n",
      "Epoch 217/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0098 - val_loss: 0.0229\n",
      "Epoch 218/400\n",
      "2687/2687 [==============================] - 1s 370us/step - loss: 0.0097 - val_loss: 0.0229\n",
      "Epoch 219/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0097 - val_loss: 0.0229\n",
      "Epoch 220/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0097 - val_loss: 0.0229\n",
      "Epoch 221/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0097 - val_loss: 0.0229\n",
      "Epoch 222/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0097 - val_loss: 0.0229\n",
      "Epoch 223/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0096 - val_loss: 0.0229\n",
      "Epoch 224/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0096 - val_loss: 0.0229\n",
      "Epoch 225/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0096 - val_loss: 0.0229\n",
      "Epoch 226/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0096 - val_loss: 0.0230\n",
      "Epoch 227/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0096 - val_loss: 0.0230\n",
      "Epoch 228/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0095 - val_loss: 0.0230\n",
      "Epoch 229/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0095 - val_loss: 0.0230\n",
      "Epoch 230/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0095 - val_loss: 0.0230\n",
      "Epoch 231/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0095 - val_loss: 0.0230\n",
      "Epoch 232/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0095 - val_loss: 0.0230\n",
      "Epoch 233/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0094 - val_loss: 0.0230\n",
      "Epoch 234/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0094 - val_loss: 0.0230\n",
      "Epoch 235/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0094 - val_loss: 0.0230\n",
      "Epoch 236/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0094 - val_loss: 0.0230\n",
      "Epoch 237/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0094 - val_loss: 0.0230\n",
      "Epoch 238/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0094 - val_loss: 0.0230\n",
      "Epoch 239/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0093 - val_loss: 0.0231\n",
      "Epoch 240/400\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.0093 - val_loss: 0.0231\n",
      "Epoch 241/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0093 - val_loss: 0.0231\n",
      "Epoch 242/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0093 - val_loss: 0.0231\n",
      "Epoch 243/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0093 - val_loss: 0.0231\n",
      "Epoch 244/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0093 - val_loss: 0.0231\n",
      "Epoch 245/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0092 - val_loss: 0.0231\n",
      "Epoch 246/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0092 - val_loss: 0.0231\n",
      "Epoch 247/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0092 - val_loss: 0.0231\n",
      "Epoch 248/400\n",
      "2687/2687 [==============================] - 1s 342us/step - loss: 0.0092 - val_loss: 0.0231\n",
      "Epoch 249/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0092 - val_loss: 0.0231\n",
      "Epoch 250/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0092 - val_loss: 0.0232\n",
      "Epoch 251/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0091 - val_loss: 0.0232\n",
      "Epoch 252/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0091 - val_loss: 0.0232\n",
      "Epoch 253/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0091 - val_loss: 0.0232\n",
      "Epoch 254/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0091 - val_loss: 0.0232\n",
      "Epoch 255/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0091 - val_loss: 0.0232\n",
      "Epoch 256/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0091 - val_loss: 0.0232\n",
      "Epoch 257/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0091 - val_loss: 0.0232\n",
      "Epoch 258/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0090 - val_loss: 0.0232\n",
      "Epoch 259/400\n",
      "2687/2687 [==============================] - 1s 378us/step - loss: 0.0090 - val_loss: 0.0232\n",
      "Epoch 260/400\n",
      "2687/2687 [==============================] - 1s 367us/step - loss: 0.0090 - val_loss: 0.0232\n",
      "Epoch 261/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0090 - val_loss: 0.0233\n",
      "Epoch 262/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0090 - val_loss: 0.0233\n",
      "Epoch 263/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0090 - val_loss: 0.0233\n",
      "Epoch 264/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0090 - val_loss: 0.0233\n",
      "Epoch 265/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "Epoch 266/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "Epoch 267/400\n",
      "2687/2687 [==============================] - 1s 350us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "Epoch 268/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "Epoch 269/400\n",
      "2687/2687 [==============================] - 1s 345us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "Epoch 270/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "Epoch 271/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0089 - val_loss: 0.0233\n",
      "Epoch 272/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 273/400\n",
      "2687/2687 [==============================] - 1s 348us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 274/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 275/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 276/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 277/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 278/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 279/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0088 - val_loss: 0.0234\n",
      "Epoch 280/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0087 - val_loss: 0.0234\n",
      "Epoch 281/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0087 - val_loss: 0.0234\n",
      "Epoch 282/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0087 - val_loss: 0.0235\n",
      "Epoch 283/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0087 - val_loss: 0.0235\n",
      "Epoch 284/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0087 - val_loss: 0.0235\n",
      "Epoch 285/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0087 - val_loss: 0.0235\n",
      "Epoch 286/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0087 - val_loss: 0.0235\n",
      "Epoch 287/400\n",
      "2687/2687 [==============================] - 1s 374us/step - loss: 0.0087 - val_loss: 0.0235\n",
      "Epoch 288/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0086 - val_loss: 0.0235\n",
      "Epoch 289/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0086 - val_loss: 0.0235\n",
      "Epoch 290/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0086 - val_loss: 0.0235\n",
      "Epoch 291/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0086 - val_loss: 0.0236\n",
      "Epoch 292/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0086 - val_loss: 0.0236\n",
      "Epoch 293/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0086 - val_loss: 0.0236\n",
      "Epoch 294/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0086 - val_loss: 0.0236\n",
      "Epoch 295/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0086 - val_loss: 0.0236\n",
      "Epoch 296/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0086 - val_loss: 0.0236\n",
      "Epoch 297/400\n",
      "2687/2687 [==============================] - 1s 348us/step - loss: 0.0085 - val_loss: 0.0236\n",
      "Epoch 298/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0085 - val_loss: 0.0236\n",
      "Epoch 299/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0085 - val_loss: 0.0236\n",
      "Epoch 300/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0085 - val_loss: 0.0236\n",
      "Epoch 301/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0085 - val_loss: 0.0237\n",
      "Epoch 302/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0085 - val_loss: 0.0237\n",
      "Epoch 303/400\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.0085 - val_loss: 0.0237\n",
      "Epoch 304/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0085 - val_loss: 0.0237\n",
      "Epoch 305/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0085 - val_loss: 0.0237\n",
      "Epoch 306/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0085 - val_loss: 0.0237\n",
      "Epoch 307/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0084 - val_loss: 0.0237\n",
      "Epoch 308/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0084 - val_loss: 0.0237\n",
      "Epoch 309/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0084 - val_loss: 0.0237\n",
      "Epoch 310/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0084 - val_loss: 0.0238\n",
      "Epoch 311/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0084 - val_loss: 0.0238\n",
      "Epoch 312/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0084 - val_loss: 0.0238\n",
      "Epoch 313/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0084 - val_loss: 0.0238\n",
      "Epoch 314/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0084 - val_loss: 0.0238\n",
      "Epoch 315/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0084 - val_loss: 0.0238\n",
      "Epoch 316/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0083 - val_loss: 0.0238\n",
      "Epoch 317/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0083 - val_loss: 0.0238\n",
      "Epoch 318/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0083 - val_loss: 0.0238\n",
      "Epoch 319/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 320/400\n",
      "2687/2687 [==============================] - 1s 361us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 321/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 322/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 323/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 324/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 325/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 326/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0083 - val_loss: 0.0239\n",
      "Epoch 327/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 328/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 329/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 330/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 331/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 332/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 333/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 334/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 335/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0082 - val_loss: 0.0240\n",
      "Epoch 336/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0082 - val_loss: 0.0241\n",
      "Epoch 337/400\n",
      "2687/2687 [==============================] - 1s 352us/step - loss: 0.0082 - val_loss: 0.0241\n",
      "Epoch 338/400\n",
      "2687/2687 [==============================] - 1s 351us/step - loss: 0.0081 - val_loss: 0.0241\n",
      "Epoch 339/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0081 - val_loss: 0.0241\n",
      "Epoch 340/400\n",
      "2687/2687 [==============================] - 1s 348us/step - loss: 0.0081 - val_loss: 0.0241\n",
      "Epoch 341/400\n",
      "2687/2687 [==============================] - 1s 355us/step - loss: 0.0081 - val_loss: 0.0241\n",
      "Epoch 342/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0081 - val_loss: 0.0241\n",
      "Epoch 343/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0081 - val_loss: 0.0241\n",
      "Epoch 344/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0081 - val_loss: 0.0241\n",
      "Epoch 345/400\n",
      "2687/2687 [==============================] - 1s 371us/step - loss: 0.0081 - val_loss: 0.0242\n",
      "Epoch 346/400\n",
      "2687/2687 [==============================] - 1s 347us/step - loss: 0.0081 - val_loss: 0.0242\n",
      "Epoch 347/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0081 - val_loss: 0.0242\n",
      "Epoch 348/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0081 - val_loss: 0.0242\n",
      "Epoch 349/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0081 - val_loss: 0.0242\n",
      "Epoch 350/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0080 - val_loss: 0.0242\n",
      "Epoch 351/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0080 - val_loss: 0.0242\n",
      "Epoch 352/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0080 - val_loss: 0.0242\n",
      "Epoch 353/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 354/400\n",
      "2687/2687 [==============================] - 1s 358us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 355/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 356/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 357/400\n",
      "2687/2687 [==============================] - 1s 355us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 358/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 359/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 360/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0080 - val_loss: 0.0243\n",
      "Epoch 361/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0080 - val_loss: 0.0244\n",
      "Epoch 362/400\n",
      "2687/2687 [==============================] - 1s 348us/step - loss: 0.0080 - val_loss: 0.0244\n",
      "Epoch 363/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0079 - val_loss: 0.0244\n",
      "Epoch 364/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0079 - val_loss: 0.0244\n",
      "Epoch 365/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0079 - val_loss: 0.0244\n",
      "Epoch 366/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0079 - val_loss: 0.0244\n",
      "Epoch 367/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0079 - val_loss: 0.0244\n",
      "Epoch 368/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0079 - val_loss: 0.0244\n",
      "Epoch 369/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0079 - val_loss: 0.0244\n",
      "Epoch 370/400\n",
      "2687/2687 [==============================] - 1s 377us/step - loss: 0.0079 - val_loss: 0.0245\n",
      "Epoch 371/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0079 - val_loss: 0.0245\n",
      "Epoch 372/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0079 - val_loss: 0.0245\n",
      "Epoch 373/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0079 - val_loss: 0.0245\n",
      "Epoch 374/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0079 - val_loss: 0.0245\n",
      "Epoch 375/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0079 - val_loss: 0.0245\n",
      "Epoch 376/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0079 - val_loss: 0.0245\n",
      "Epoch 377/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0078 - val_loss: 0.0245\n",
      "Epoch 378/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 379/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 380/400\n",
      "2687/2687 [==============================] - 1s 369us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 381/400\n",
      "2687/2687 [==============================] - 1s 360us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 382/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 383/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 384/400\n",
      "2687/2687 [==============================] - 1s 362us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 385/400\n",
      "2687/2687 [==============================] - 1s 363us/step - loss: 0.0078 - val_loss: 0.0246\n",
      "Epoch 386/400\n",
      "2687/2687 [==============================] - 1s 359us/step - loss: 0.0078 - val_loss: 0.0247\n",
      "Epoch 387/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0078 - val_loss: 0.0247\n",
      "Epoch 388/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0078 - val_loss: 0.0247\n",
      "Epoch 389/400\n",
      "2687/2687 [==============================] - 1s 366us/step - loss: 0.0078 - val_loss: 0.0247\n",
      "Epoch 390/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0078 - val_loss: 0.0247\n",
      "Epoch 391/400\n",
      "2687/2687 [==============================] - 1s 357us/step - loss: 0.0078 - val_loss: 0.0247\n",
      "Epoch 392/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0077 - val_loss: 0.0247\n",
      "Epoch 393/400\n",
      "2687/2687 [==============================] - 1s 356us/step - loss: 0.0077 - val_loss: 0.0247\n",
      "Epoch 394/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0077 - val_loss: 0.0248\n",
      "Epoch 395/400\n",
      "2687/2687 [==============================] - 1s 365us/step - loss: 0.0077 - val_loss: 0.0248\n",
      "Epoch 396/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0077 - val_loss: 0.0248\n",
      "Epoch 397/400\n",
      "2687/2687 [==============================] - 1s 368us/step - loss: 0.0077 - val_loss: 0.0248\n",
      "Epoch 398/400\n",
      "2687/2687 [==============================] - 1s 372us/step - loss: 0.0077 - val_loss: 0.0248\n",
      "Epoch 399/400\n",
      "2687/2687 [==============================] - 1s 354us/step - loss: 0.0077 - val_loss: 0.0248\n",
      "Epoch 400/400\n",
      "2687/2687 [==============================] - 1s 353us/step - loss: 0.0077 - val_loss: 0.0248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29ccee60f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(feats, target, batch_size=5, epochs=400, verbose=1, validation_split=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fibrosis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv(r'C:\\Users\\My PC\\Desktop\\Machine Learning\\Data Engineer\\hcv.csv')\n",
    "y=pd.read_csv(r'C:\\Users\\My PC\\Desktop\\Machine Learning\\Data Engineer\\hcvt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)\n",
    "seed=1\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 3)                 87        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 91\n",
      "Trainable params: 91\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier=Sequential()\n",
    "classifier.add(Dense(3,activation='tanh',input_dim=X_train.shape[1]))\n",
    "classifier.add(Dense(1,activation='sigmoid'))\n",
    "classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 997 samples, validate on 111 samples\n",
      "Epoch 1/100\n",
      "997/997 [==============================] - 2s 2ms/step - loss: 0.7972 - accuracy: 0.4714 - val_loss: 0.7460 - val_accuracy: 0.5676\n",
      "Epoch 2/100\n",
      "997/997 [==============================] - 0s 245us/step - loss: 0.7795 - accuracy: 0.4764 - val_loss: 0.7343 - val_accuracy: 0.5676\n",
      "Epoch 3/100\n",
      "997/997 [==============================] - 0s 249us/step - loss: 0.7653 - accuracy: 0.4794 - val_loss: 0.7248 - val_accuracy: 0.5676\n",
      "Epoch 4/100\n",
      "997/997 [==============================] - 0s 249us/step - loss: 0.7536 - accuracy: 0.4875 - val_loss: 0.7169 - val_accuracy: 0.5766\n",
      "Epoch 5/100\n",
      "997/997 [==============================] - 0s 245us/step - loss: 0.7441 - accuracy: 0.4945 - val_loss: 0.7104 - val_accuracy: 0.5766\n",
      "Epoch 6/100\n",
      "997/997 [==============================] - 0s 245us/step - loss: 0.7363 - accuracy: 0.5035 - val_loss: 0.7050 - val_accuracy: 0.5676\n",
      "Epoch 7/100\n",
      "997/997 [==============================] - 0s 249us/step - loss: 0.7297 - accuracy: 0.5075 - val_loss: 0.7005 - val_accuracy: 0.5586\n",
      "Epoch 8/100\n",
      "997/997 [==============================] - 0s 261us/step - loss: 0.7243 - accuracy: 0.5115 - val_loss: 0.6967 - val_accuracy: 0.5586\n",
      "Epoch 9/100\n",
      "997/997 [==============================] - 0s 249us/step - loss: 0.7197 - accuracy: 0.5206 - val_loss: 0.6935 - val_accuracy: 0.5495\n",
      "Epoch 10/100\n",
      "997/997 [==============================] - 0s 205us/step - loss: 0.7158 - accuracy: 0.5236 - val_loss: 0.6908 - val_accuracy: 0.5495\n",
      "Epoch 11/100\n",
      "997/997 [==============================] - 0s 225us/step - loss: 0.7125 - accuracy: 0.5256 - val_loss: 0.6886 - val_accuracy: 0.5766\n",
      "Epoch 12/100\n",
      "997/997 [==============================] - 0s 221us/step - loss: 0.7097 - accuracy: 0.5266 - val_loss: 0.6866 - val_accuracy: 0.5766\n",
      "Epoch 13/100\n",
      "997/997 [==============================] - 0s 205us/step - loss: 0.7072 - accuracy: 0.5246 - val_loss: 0.6849 - val_accuracy: 0.5766\n",
      "Epoch 14/100\n",
      "997/997 [==============================] - 0s 209us/step - loss: 0.7051 - accuracy: 0.5276 - val_loss: 0.6835 - val_accuracy: 0.5766\n",
      "Epoch 15/100\n",
      "997/997 [==============================] - 0s 193us/step - loss: 0.7032 - accuracy: 0.5216 - val_loss: 0.6823 - val_accuracy: 0.5766\n",
      "Epoch 16/100\n",
      "997/997 [==============================] - 0s 193us/step - loss: 0.7016 - accuracy: 0.5236 - val_loss: 0.6812 - val_accuracy: 0.5766\n",
      "Epoch 17/100\n",
      "997/997 [==============================] - 0s 185us/step - loss: 0.7002 - accuracy: 0.5236 - val_loss: 0.6803 - val_accuracy: 0.5766\n",
      "Epoch 18/100\n",
      "997/997 [==============================] - 0s 181us/step - loss: 0.6990 - accuracy: 0.5236 - val_loss: 0.6795 - val_accuracy: 0.5676\n",
      "Epoch 19/100\n",
      "997/997 [==============================] - 0s 189us/step - loss: 0.6979 - accuracy: 0.5246 - val_loss: 0.6788 - val_accuracy: 0.5676\n",
      "Epoch 20/100\n",
      "997/997 [==============================] - 0s 152us/step - loss: 0.6969 - accuracy: 0.5266 - val_loss: 0.6782 - val_accuracy: 0.5766\n",
      "Epoch 21/100\n",
      "997/997 [==============================] - 0s 156us/step - loss: 0.6960 - accuracy: 0.5266 - val_loss: 0.6777 - val_accuracy: 0.5856\n",
      "Epoch 22/100\n",
      "997/997 [==============================] - 0s 165us/step - loss: 0.6952 - accuracy: 0.5256 - val_loss: 0.6772 - val_accuracy: 0.5856\n",
      "Epoch 23/100\n",
      "997/997 [==============================] - 0s 181us/step - loss: 0.6945 - accuracy: 0.5266 - val_loss: 0.6768 - val_accuracy: 0.5766\n",
      "Epoch 24/100\n",
      "997/997 [==============================] - 0s 165us/step - loss: 0.6938 - accuracy: 0.5266 - val_loss: 0.6765 - val_accuracy: 0.5766\n",
      "Epoch 25/100\n",
      "997/997 [==============================] - 0s 156us/step - loss: 0.6932 - accuracy: 0.5256 - val_loss: 0.6762 - val_accuracy: 0.5766\n",
      "Epoch 26/100\n",
      "997/997 [==============================] - 0s 156us/step - loss: 0.6927 - accuracy: 0.5256 - val_loss: 0.6759 - val_accuracy: 0.5856\n",
      "Epoch 27/100\n",
      "997/997 [==============================] - 0s 152us/step - loss: 0.6922 - accuracy: 0.5276 - val_loss: 0.6756 - val_accuracy: 0.5856\n",
      "Epoch 28/100\n",
      "997/997 [==============================] - 0s 148us/step - loss: 0.6918 - accuracy: 0.5306 - val_loss: 0.6754 - val_accuracy: 0.5856\n",
      "Epoch 29/100\n",
      "997/997 [==============================] - 0s 132us/step - loss: 0.6913 - accuracy: 0.5316 - val_loss: 0.6752 - val_accuracy: 0.5856\n",
      "Epoch 30/100\n",
      "997/997 [==============================] - 0s 148us/step - loss: 0.6910 - accuracy: 0.5306 - val_loss: 0.6750 - val_accuracy: 0.5856\n",
      "Epoch 31/100\n",
      "997/997 [==============================] - 0s 144us/step - loss: 0.6906 - accuracy: 0.5296 - val_loss: 0.6749 - val_accuracy: 0.5856\n",
      "Epoch 32/100\n",
      "997/997 [==============================] - 0s 144us/step - loss: 0.6903 - accuracy: 0.5326 - val_loss: 0.6747 - val_accuracy: 0.5856\n",
      "Epoch 33/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6899 - accuracy: 0.5336 - val_loss: 0.6746 - val_accuracy: 0.5946\n",
      "Epoch 34/100\n",
      "997/997 [==============================] - 0s 140us/step - loss: 0.6896 - accuracy: 0.5386 - val_loss: 0.6745 - val_accuracy: 0.5946\n",
      "Epoch 35/100\n",
      "997/997 [==============================] - 0s 140us/step - loss: 0.6894 - accuracy: 0.5396 - val_loss: 0.6744 - val_accuracy: 0.6036\n",
      "Epoch 36/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6891 - accuracy: 0.5406 - val_loss: 0.6743 - val_accuracy: 0.6126\n",
      "Epoch 37/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6889 - accuracy: 0.5426 - val_loss: 0.6742 - val_accuracy: 0.6036\n",
      "Epoch 38/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6886 - accuracy: 0.5426 - val_loss: 0.6741 - val_accuracy: 0.6036\n",
      "Epoch 39/100\n",
      "997/997 [==============================] - 0s 120us/step - loss: 0.6884 - accuracy: 0.5436 - val_loss: 0.6740 - val_accuracy: 0.6036\n",
      "Epoch 40/100\n",
      "997/997 [==============================] - 0s 124us/step - loss: 0.6882 - accuracy: 0.5486 - val_loss: 0.6739 - val_accuracy: 0.6036\n",
      "Epoch 41/100\n",
      "997/997 [==============================] - 0s 134us/step - loss: 0.6879 - accuracy: 0.5507 - val_loss: 0.6738 - val_accuracy: 0.6036\n",
      "Epoch 42/100\n",
      "997/997 [==============================] - 0s 128us/step - loss: 0.6877 - accuracy: 0.5527 - val_loss: 0.6738 - val_accuracy: 0.6036\n",
      "Epoch 43/100\n",
      "997/997 [==============================] - 0s 144us/step - loss: 0.6875 - accuracy: 0.5527 - val_loss: 0.6737 - val_accuracy: 0.5946\n",
      "Epoch 44/100\n",
      "997/997 [==============================] - 0s 132us/step - loss: 0.6873 - accuracy: 0.5527 - val_loss: 0.6736 - val_accuracy: 0.6036\n",
      "Epoch 45/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6871 - accuracy: 0.5537 - val_loss: 0.6736 - val_accuracy: 0.6126\n",
      "Epoch 46/100\n",
      "997/997 [==============================] - 0s 124us/step - loss: 0.6870 - accuracy: 0.5547 - val_loss: 0.6735 - val_accuracy: 0.6036\n",
      "Epoch 47/100\n",
      "997/997 [==============================] - 0s 132us/step - loss: 0.6868 - accuracy: 0.5547 - val_loss: 0.6734 - val_accuracy: 0.6036\n",
      "Epoch 48/100\n",
      "997/997 [==============================] - 0s 124us/step - loss: 0.6866 - accuracy: 0.5537 - val_loss: 0.6734 - val_accuracy: 0.6036\n",
      "Epoch 49/100\n",
      "997/997 [==============================] - 0s 120us/step - loss: 0.6864 - accuracy: 0.5567 - val_loss: 0.6733 - val_accuracy: 0.6036\n",
      "Epoch 50/100\n",
      "997/997 [==============================] - 0s 124us/step - loss: 0.6862 - accuracy: 0.5597 - val_loss: 0.6733 - val_accuracy: 0.6036\n",
      "Epoch 51/100\n",
      "997/997 [==============================] - 0s 124us/step - loss: 0.6861 - accuracy: 0.5627 - val_loss: 0.6732 - val_accuracy: 0.6036\n",
      "Epoch 52/100\n",
      "997/997 [==============================] - 0s 128us/step - loss: 0.6859 - accuracy: 0.5617 - val_loss: 0.6732 - val_accuracy: 0.6036\n",
      "Epoch 53/100\n",
      "997/997 [==============================] - 0s 120us/step - loss: 0.6857 - accuracy: 0.5617 - val_loss: 0.6731 - val_accuracy: 0.6036\n",
      "Epoch 54/100\n",
      "997/997 [==============================] - 0s 120us/step - loss: 0.6855 - accuracy: 0.5617 - val_loss: 0.6731 - val_accuracy: 0.6036\n",
      "Epoch 55/100\n",
      "997/997 [==============================] - 0s 120us/step - loss: 0.6854 - accuracy: 0.5617 - val_loss: 0.6730 - val_accuracy: 0.6036\n",
      "Epoch 56/100\n",
      "997/997 [==============================] - 0s 116us/step - loss: 0.6852 - accuracy: 0.5627 - val_loss: 0.6730 - val_accuracy: 0.5946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "997/997 [==============================] - 0s 128us/step - loss: 0.6850 - accuracy: 0.5637 - val_loss: 0.6729 - val_accuracy: 0.5946\n",
      "Epoch 58/100\n",
      "997/997 [==============================] - 0s 116us/step - loss: 0.6849 - accuracy: 0.5607 - val_loss: 0.6729 - val_accuracy: 0.5946\n",
      "Epoch 59/100\n",
      "997/997 [==============================] - 0s 112us/step - loss: 0.6847 - accuracy: 0.5627 - val_loss: 0.6728 - val_accuracy: 0.5946\n",
      "Epoch 60/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6845 - accuracy: 0.5607 - val_loss: 0.6728 - val_accuracy: 0.5946\n",
      "Epoch 61/100\n",
      "997/997 [==============================] - 0s 116us/step - loss: 0.6844 - accuracy: 0.5597 - val_loss: 0.6728 - val_accuracy: 0.5946\n",
      "Epoch 62/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6842 - accuracy: 0.5607 - val_loss: 0.6727 - val_accuracy: 0.5946\n",
      "Epoch 63/100\n",
      "997/997 [==============================] - 0s 112us/step - loss: 0.6840 - accuracy: 0.5617 - val_loss: 0.6727 - val_accuracy: 0.5946\n",
      "Epoch 64/100\n",
      "997/997 [==============================] - 0s 112us/step - loss: 0.6839 - accuracy: 0.5607 - val_loss: 0.6726 - val_accuracy: 0.5946\n",
      "Epoch 65/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6837 - accuracy: 0.5597 - val_loss: 0.6726 - val_accuracy: 0.5946\n",
      "Epoch 66/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6835 - accuracy: 0.5607 - val_loss: 0.6726 - val_accuracy: 0.5946\n",
      "Epoch 67/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6834 - accuracy: 0.5617 - val_loss: 0.6725 - val_accuracy: 0.5946\n",
      "Epoch 68/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6832 - accuracy: 0.5617 - val_loss: 0.6725 - val_accuracy: 0.5946\n",
      "Epoch 69/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6830 - accuracy: 0.5607 - val_loss: 0.6725 - val_accuracy: 0.5856\n",
      "Epoch 70/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6829 - accuracy: 0.5597 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 71/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6827 - accuracy: 0.5607 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 72/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6825 - accuracy: 0.5607 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 73/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6823 - accuracy: 0.5617 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 74/100\n",
      "997/997 [==============================] - 0s 112us/step - loss: 0.6822 - accuracy: 0.5617 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 75/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6820 - accuracy: 0.5627 - val_loss: 0.6723 - val_accuracy: 0.5856\n",
      "Epoch 76/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6818 - accuracy: 0.5637 - val_loss: 0.6723 - val_accuracy: 0.5856\n",
      "Epoch 77/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6817 - accuracy: 0.5667 - val_loss: 0.6723 - val_accuracy: 0.5856\n",
      "Epoch 78/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6815 - accuracy: 0.5697 - val_loss: 0.6723 - val_accuracy: 0.5856\n",
      "Epoch 79/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6813 - accuracy: 0.5707 - val_loss: 0.6723 - val_accuracy: 0.5946\n",
      "Epoch 80/100\n",
      "997/997 [==============================] - 0s 112us/step - loss: 0.6811 - accuracy: 0.5717 - val_loss: 0.6723 - val_accuracy: 0.5946\n",
      "Epoch 81/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6810 - accuracy: 0.5717 - val_loss: 0.6723 - val_accuracy: 0.5946\n",
      "Epoch 82/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6808 - accuracy: 0.5707 - val_loss: 0.6723 - val_accuracy: 0.5946\n",
      "Epoch 83/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6806 - accuracy: 0.5727 - val_loss: 0.6723 - val_accuracy: 0.5856\n",
      "Epoch 84/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6805 - accuracy: 0.5737 - val_loss: 0.6723 - val_accuracy: 0.5856\n",
      "Epoch 85/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6803 - accuracy: 0.5747 - val_loss: 0.6723 - val_accuracy: 0.5856\n",
      "Epoch 86/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6801 - accuracy: 0.5727 - val_loss: 0.6723 - val_accuracy: 0.5766\n",
      "Epoch 87/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6799 - accuracy: 0.5707 - val_loss: 0.6723 - val_accuracy: 0.5766\n",
      "Epoch 88/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6798 - accuracy: 0.5707 - val_loss: 0.6723 - val_accuracy: 0.5766\n",
      "Epoch 89/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6796 - accuracy: 0.5677 - val_loss: 0.6723 - val_accuracy: 0.5766\n",
      "Epoch 90/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6794 - accuracy: 0.5687 - val_loss: 0.6724 - val_accuracy: 0.5766\n",
      "Epoch 91/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6792 - accuracy: 0.5707 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 92/100\n",
      "997/997 [==============================] - 0s 100us/step - loss: 0.6791 - accuracy: 0.5707 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 93/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6789 - accuracy: 0.5707 - val_loss: 0.6724 - val_accuracy: 0.5856\n",
      "Epoch 94/100\n",
      "997/997 [==============================] - 0s 112us/step - loss: 0.6787 - accuracy: 0.5707 - val_loss: 0.6725 - val_accuracy: 0.5946\n",
      "Epoch 95/100\n",
      "997/997 [==============================] - 0s 120us/step - loss: 0.6786 - accuracy: 0.5697 - val_loss: 0.6725 - val_accuracy: 0.5946\n",
      "Epoch 96/100\n",
      "997/997 [==============================] - 0s 116us/step - loss: 0.6784 - accuracy: 0.5697 - val_loss: 0.6725 - val_accuracy: 0.5946\n",
      "Epoch 97/100\n",
      "997/997 [==============================] - 0s 108us/step - loss: 0.6782 - accuracy: 0.5697 - val_loss: 0.6725 - val_accuracy: 0.5946\n",
      "Epoch 98/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6780 - accuracy: 0.5687 - val_loss: 0.6726 - val_accuracy: 0.5856\n",
      "Epoch 99/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6779 - accuracy: 0.5657 - val_loss: 0.6726 - val_accuracy: 0.5766\n",
      "Epoch 100/100\n",
      "997/997 [==============================] - 0s 104us/step - loss: 0.6777 - accuracy: 0.5667 - val_loss: 0.6726 - val_accuracy: 0.5766\n"
     ]
    }
   ],
   "source": [
    "history=classifier.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_split=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29cffb5e10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy on training set = 57.472%\n",
      "Best Accuracy on validation set = 61.261%\n",
      "277/277 [==============================] - 0s 83us/step\n",
      "The loss on the test set is 0.7101 and the accuracy is 49.458%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Accuracy on training set = {max(history.history['accuracy'])*100:.3f}%\")\n",
    "print(f\"Best Accuracy on validation set = {max(history.history['val_accuracy'])*100:.3f}%\") \n",
    "test_loss, test_acc = classifier.evaluate(X_test, y_test['AdvancedFibrosis'])\n",
    "print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 3)                 87        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 8         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 112\n",
      "Trainable params: 112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(units = 4, activation = 'tanh', input_dim = X_train.shape[1]))\n",
    "classifier.add(Dense(units = 2, activation = 'tanh'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 997 samples, validate on 111 samples\n",
      "Epoch 1/100\n",
      "997/997 [==============================] - 2s 2ms/step - loss: 0.6972 - accuracy: 0.4975 - val_loss: 0.7073 - val_accuracy: 0.4054\n",
      "Epoch 2/100\n",
      "997/997 [==============================] - 0s 301us/step - loss: 0.6964 - accuracy: 0.4975 - val_loss: 0.7046 - val_accuracy: 0.4054\n",
      "Epoch 3/100\n",
      "997/997 [==============================] - 0s 297us/step - loss: 0.6958 - accuracy: 0.4975 - val_loss: 0.7024 - val_accuracy: 0.4054\n",
      "Epoch 4/100\n",
      "997/997 [==============================] - 0s 276us/step - loss: 0.6955 - accuracy: 0.4975 - val_loss: 0.7008 - val_accuracy: 0.4054\n",
      "Epoch 5/100\n",
      "997/997 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.50 - 0s 280us/step - loss: 0.6952 - accuracy: 0.4975 - val_loss: 0.6994 - val_accuracy: 0.4054\n",
      "Epoch 6/100\n",
      "997/997 [==============================] - 0s 283us/step - loss: 0.6950 - accuracy: 0.4935 - val_loss: 0.6983 - val_accuracy: 0.4054\n",
      "Epoch 7/100\n",
      "997/997 [==============================] - 0s 264us/step - loss: 0.6949 - accuracy: 0.4804 - val_loss: 0.6975 - val_accuracy: 0.4324\n",
      "Epoch 8/100\n",
      "997/997 [==============================] - 0s 247us/step - loss: 0.6948 - accuracy: 0.4624 - val_loss: 0.6968 - val_accuracy: 0.4595\n",
      "Epoch 9/100\n",
      "997/997 [==============================] - 0s 234us/step - loss: 0.6947 - accuracy: 0.4493 - val_loss: 0.6962 - val_accuracy: 0.4234\n",
      "Epoch 10/100\n",
      "997/997 [==============================] - 0s 202us/step - loss: 0.6947 - accuracy: 0.4273 - val_loss: 0.6957 - val_accuracy: 0.3964\n",
      "Epoch 11/100\n",
      "997/997 [==============================] - 0s 210us/step - loss: 0.6946 - accuracy: 0.4263 - val_loss: 0.6953 - val_accuracy: 0.4144\n",
      "Epoch 12/100\n",
      "997/997 [==============================] - 0s 214us/step - loss: 0.6946 - accuracy: 0.4303 - val_loss: 0.6950 - val_accuracy: 0.4414\n",
      "Epoch 13/100\n",
      "997/997 [==============================] - 0s 218us/step - loss: 0.6946 - accuracy: 0.4233 - val_loss: 0.6947 - val_accuracy: 0.4054\n",
      "Epoch 14/100\n",
      "997/997 [==============================] - 0s 210us/step - loss: 0.6946 - accuracy: 0.4243 - val_loss: 0.6945 - val_accuracy: 0.3874\n",
      "Epoch 15/100\n",
      "997/997 [==============================] - 0s 204us/step - loss: 0.6945 - accuracy: 0.4193 - val_loss: 0.6943 - val_accuracy: 0.3964\n",
      "Epoch 16/100\n",
      "997/997 [==============================] - 0s 200us/step - loss: 0.6945 - accuracy: 0.4293 - val_loss: 0.6942 - val_accuracy: 0.3964\n",
      "Epoch 17/100\n",
      "997/997 [==============================] - 0s 185us/step - loss: 0.6945 - accuracy: 0.4333 - val_loss: 0.6940 - val_accuracy: 0.4054\n",
      "Epoch 18/100\n",
      "997/997 [==============================] - 0s 183us/step - loss: 0.6945 - accuracy: 0.4323 - val_loss: 0.6939 - val_accuracy: 0.4144\n",
      "Epoch 19/100\n",
      "997/997 [==============================] - 0s 185us/step - loss: 0.6945 - accuracy: 0.4333 - val_loss: 0.6938 - val_accuracy: 0.4324\n",
      "Epoch 20/100\n",
      "997/997 [==============================] - 0s 142us/step - loss: 0.6945 - accuracy: 0.4413 - val_loss: 0.6938 - val_accuracy: 0.4505\n",
      "Epoch 21/100\n",
      "997/997 [==============================] - 0s 174us/step - loss: 0.6944 - accuracy: 0.4383 - val_loss: 0.6937 - val_accuracy: 0.4414\n",
      "Epoch 22/100\n",
      "997/997 [==============================] - 0s 217us/step - loss: 0.6944 - accuracy: 0.4433 - val_loss: 0.6936 - val_accuracy: 0.4414\n",
      "Epoch 23/100\n",
      "997/997 [==============================] - 0s 178us/step - loss: 0.6944 - accuracy: 0.4534 - val_loss: 0.6936 - val_accuracy: 0.4505\n",
      "Epoch 24/100\n",
      "997/997 [==============================] - 0s 166us/step - loss: 0.6944 - accuracy: 0.4524 - val_loss: 0.6935 - val_accuracy: 0.4595\n",
      "Epoch 25/100\n",
      "997/997 [==============================] - 0s 175us/step - loss: 0.6944 - accuracy: 0.4524 - val_loss: 0.6935 - val_accuracy: 0.4775\n",
      "Epoch 26/100\n",
      "997/997 [==============================] - 0s 165us/step - loss: 0.6944 - accuracy: 0.4504 - val_loss: 0.6934 - val_accuracy: 0.4775\n",
      "Epoch 27/100\n",
      "997/997 [==============================] - 0s 158us/step - loss: 0.6944 - accuracy: 0.4483 - val_loss: 0.6934 - val_accuracy: 0.4865\n",
      "Epoch 28/100\n",
      "997/997 [==============================] - 0s 161us/step - loss: 0.6943 - accuracy: 0.4483 - val_loss: 0.6934 - val_accuracy: 0.4865\n",
      "Epoch 29/100\n",
      "997/997 [==============================] - 0s 166us/step - loss: 0.6943 - accuracy: 0.4564 - val_loss: 0.6934 - val_accuracy: 0.4775\n",
      "Epoch 30/100\n",
      "997/997 [==============================] - 0s 153us/step - loss: 0.6943 - accuracy: 0.4574 - val_loss: 0.6933 - val_accuracy: 0.4775\n",
      "Epoch 31/100\n",
      "997/997 [==============================] - 0s 155us/step - loss: 0.6943 - accuracy: 0.4614 - val_loss: 0.6933 - val_accuracy: 0.4865\n",
      "Epoch 32/100\n",
      "997/997 [==============================] - 0s 156us/step - loss: 0.6943 - accuracy: 0.4634 - val_loss: 0.6933 - val_accuracy: 0.4865\n",
      "Epoch 33/100\n",
      "997/997 [==============================] - 0s 160us/step - loss: 0.6943 - accuracy: 0.4584 - val_loss: 0.6933 - val_accuracy: 0.4865\n",
      "Epoch 34/100\n",
      "997/997 [==============================] - 0s 154us/step - loss: 0.6943 - accuracy: 0.4594 - val_loss: 0.6932 - val_accuracy: 0.4865\n",
      "Epoch 35/100\n",
      "997/997 [==============================] - 0s 150us/step - loss: 0.6942 - accuracy: 0.4614 - val_loss: 0.6932 - val_accuracy: 0.4865\n",
      "Epoch 36/100\n",
      "997/997 [==============================] - 0s 145us/step - loss: 0.6942 - accuracy: 0.4644 - val_loss: 0.6932 - val_accuracy: 0.4865\n",
      "Epoch 37/100\n",
      "997/997 [==============================] - 0s 150us/step - loss: 0.6942 - accuracy: 0.4654 - val_loss: 0.6932 - val_accuracy: 0.4865\n",
      "Epoch 38/100\n",
      "997/997 [==============================] - 0s 151us/step - loss: 0.6942 - accuracy: 0.4704 - val_loss: 0.6932 - val_accuracy: 0.4955\n",
      "Epoch 39/100\n",
      "997/997 [==============================] - 0s 147us/step - loss: 0.6942 - accuracy: 0.4734 - val_loss: 0.6932 - val_accuracy: 0.4955\n",
      "Epoch 40/100\n",
      "997/997 [==============================] - 0s 146us/step - loss: 0.6942 - accuracy: 0.4744 - val_loss: 0.6932 - val_accuracy: 0.4955\n",
      "Epoch 41/100\n",
      "997/997 [==============================] - 0s 153us/step - loss: 0.6942 - accuracy: 0.4744 - val_loss: 0.6931 - val_accuracy: 0.4865\n",
      "Epoch 42/100\n",
      "997/997 [==============================] - 0s 146us/step - loss: 0.6942 - accuracy: 0.4754 - val_loss: 0.6931 - val_accuracy: 0.4955\n",
      "Epoch 43/100\n",
      "997/997 [==============================] - 0s 143us/step - loss: 0.6942 - accuracy: 0.4774 - val_loss: 0.6931 - val_accuracy: 0.4955\n",
      "Epoch 44/100\n",
      "997/997 [==============================] - 0s 149us/step - loss: 0.6941 - accuracy: 0.4784 - val_loss: 0.6931 - val_accuracy: 0.5045\n",
      "Epoch 45/100\n",
      "997/997 [==============================] - 0s 145us/step - loss: 0.6941 - accuracy: 0.4774 - val_loss: 0.6931 - val_accuracy: 0.5045\n",
      "Epoch 46/100\n",
      "997/997 [==============================] - 0s 145us/step - loss: 0.6941 - accuracy: 0.4794 - val_loss: 0.6931 - val_accuracy: 0.5045\n",
      "Epoch 47/100\n",
      "997/997 [==============================] - 0s 143us/step - loss: 0.6941 - accuracy: 0.4804 - val_loss: 0.6931 - val_accuracy: 0.5045\n",
      "Epoch 48/100\n",
      "997/997 [==============================] - 0s 147us/step - loss: 0.6941 - accuracy: 0.4794 - val_loss: 0.6930 - val_accuracy: 0.5045\n",
      "Epoch 49/100\n",
      "997/997 [==============================] - 0s 153us/step - loss: 0.6941 - accuracy: 0.4804 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 50/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6941 - accuracy: 0.4794 - val_loss: 0.6930 - val_accuracy: 0.5045\n",
      "Epoch 51/100\n",
      "997/997 [==============================] - 0s 131us/step - loss: 0.6941 - accuracy: 0.4824 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 52/100\n",
      "997/997 [==============================] - 0s 142us/step - loss: 0.6941 - accuracy: 0.4845 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 53/100\n",
      "997/997 [==============================] - 0s 147us/step - loss: 0.6941 - accuracy: 0.4895 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 54/100\n",
      "997/997 [==============================] - 0s 142us/step - loss: 0.6940 - accuracy: 0.4905 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 55/100\n",
      "997/997 [==============================] - 0s 151us/step - loss: 0.6940 - accuracy: 0.4905 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 56/100\n",
      "997/997 [==============================] - 0s 146us/step - loss: 0.6940 - accuracy: 0.4915 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 57/100\n",
      "997/997 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.48 - 0s 140us/step - loss: 0.6940 - accuracy: 0.4945 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 58/100\n",
      "997/997 [==============================] - 0s 130us/step - loss: 0.6940 - accuracy: 0.4935 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 59/100\n",
      "997/997 [==============================] - 0s 134us/step - loss: 0.6940 - accuracy: 0.4915 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 60/100\n",
      "997/997 [==============================] - 0s 127us/step - loss: 0.6940 - accuracy: 0.4925 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 61/100\n",
      "997/997 [==============================] - 0s 130us/step - loss: 0.6940 - accuracy: 0.4935 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 62/100\n",
      "997/997 [==============================] - 0s 134us/step - loss: 0.6940 - accuracy: 0.4935 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 63/100\n",
      "997/997 [==============================] - 0s 140us/step - loss: 0.6940 - accuracy: 0.4935 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 64/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6939 - accuracy: 0.4955 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 65/100\n",
      "997/997 [==============================] - 0s 164us/step - loss: 0.6939 - accuracy: 0.4945 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 66/100\n",
      "997/997 [==============================] - 0s 130us/step - loss: 0.6939 - accuracy: 0.4935 - val_loss: 0.6928 - val_accuracy: 0.5315\n",
      "Epoch 67/100\n",
      "997/997 [==============================] - 0s 131us/step - loss: 0.6939 - accuracy: 0.4915 - val_loss: 0.6928 - val_accuracy: 0.5315\n",
      "Epoch 68/100\n",
      "997/997 [==============================] - 0s 131us/step - loss: 0.6939 - accuracy: 0.4925 - val_loss: 0.6928 - val_accuracy: 0.5315\n",
      "Epoch 69/100\n",
      "997/997 [==============================] - 0s 127us/step - loss: 0.6939 - accuracy: 0.4895 - val_loss: 0.6928 - val_accuracy: 0.5315\n",
      "Epoch 70/100\n",
      "997/997 [==============================] - 0s 139us/step - loss: 0.6939 - accuracy: 0.4895 - val_loss: 0.6928 - val_accuracy: 0.5495\n",
      "Epoch 71/100\n",
      "997/997 [==============================] - 0s 133us/step - loss: 0.6939 - accuracy: 0.4935 - val_loss: 0.6928 - val_accuracy: 0.5495\n",
      "Epoch 72/100\n",
      "997/997 [==============================] - 0s 139us/step - loss: 0.6939 - accuracy: 0.4935 - val_loss: 0.6928 - val_accuracy: 0.5495\n",
      "Epoch 73/100\n",
      "997/997 [==============================] - 0s 133us/step - loss: 0.6939 - accuracy: 0.4945 - val_loss: 0.6928 - val_accuracy: 0.5495\n",
      "Epoch 74/100\n",
      "997/997 [==============================] - 0s 133us/step - loss: 0.6939 - accuracy: 0.4915 - val_loss: 0.6928 - val_accuracy: 0.5495\n",
      "Epoch 75/100\n",
      "997/997 [==============================] - 0s 140us/step - loss: 0.6939 - accuracy: 0.4915 - val_loss: 0.6928 - val_accuracy: 0.5495\n",
      "Epoch 76/100\n",
      "997/997 [==============================] - 0s 139us/step - loss: 0.6938 - accuracy: 0.4905 - val_loss: 0.6927 - val_accuracy: 0.5495\n",
      "Epoch 77/100\n",
      "997/997 [==============================] - 0s 133us/step - loss: 0.6938 - accuracy: 0.4905 - val_loss: 0.6927 - val_accuracy: 0.5495\n",
      "Epoch 78/100\n",
      "997/997 [==============================] - 0s 139us/step - loss: 0.6938 - accuracy: 0.4915 - val_loss: 0.6927 - val_accuracy: 0.5495\n",
      "Epoch 79/100\n",
      "997/997 [==============================] - 0s 123us/step - loss: 0.6938 - accuracy: 0.4895 - val_loss: 0.6927 - val_accuracy: 0.5495\n",
      "Epoch 80/100\n",
      "997/997 [==============================] - 0s 137us/step - loss: 0.6938 - accuracy: 0.4915 - val_loss: 0.6927 - val_accuracy: 0.5586\n",
      "Epoch 81/100\n",
      "997/997 [==============================] - 0s 131us/step - loss: 0.6938 - accuracy: 0.4905 - val_loss: 0.6927 - val_accuracy: 0.5586\n",
      "Epoch 82/100\n",
      "997/997 [==============================] - 0s 140us/step - loss: 0.6938 - accuracy: 0.4895 - val_loss: 0.6927 - val_accuracy: 0.5586\n",
      "Epoch 83/100\n",
      "997/997 [==============================] - 0s 158us/step - loss: 0.6938 - accuracy: 0.4905 - val_loss: 0.6927 - val_accuracy: 0.5586\n",
      "Epoch 84/100\n",
      "997/997 [==============================] - 0s 156us/step - loss: 0.6938 - accuracy: 0.4915 - val_loss: 0.6927 - val_accuracy: 0.5586\n",
      "Epoch 85/100\n",
      "997/997 [==============================] - 0s 137us/step - loss: 0.6938 - accuracy: 0.4895 - val_loss: 0.6927 - val_accuracy: 0.5495\n",
      "Epoch 86/100\n",
      "997/997 [==============================] - 0s 138us/step - loss: 0.6938 - accuracy: 0.4915 - val_loss: 0.6927 - val_accuracy: 0.5586\n",
      "Epoch 87/100\n",
      "997/997 [==============================] - 0s 146us/step - loss: 0.6938 - accuracy: 0.4925 - val_loss: 0.6927 - val_accuracy: 0.5586\n",
      "Epoch 88/100\n",
      "997/997 [==============================] - 0s 148us/step - loss: 0.6938 - accuracy: 0.4925 - val_loss: 0.6926 - val_accuracy: 0.5676\n",
      "Epoch 89/100\n",
      "997/997 [==============================] - 0s 139us/step - loss: 0.6938 - accuracy: 0.4935 - val_loss: 0.6926 - val_accuracy: 0.5676\n",
      "Epoch 90/100\n",
      "997/997 [==============================] - 0s 150us/step - loss: 0.6937 - accuracy: 0.4935 - val_loss: 0.6926 - val_accuracy: 0.5676\n",
      "Epoch 91/100\n",
      "997/997 [==============================] - 0s 149us/step - loss: 0.6937 - accuracy: 0.4925 - val_loss: 0.6926 - val_accuracy: 0.5676\n",
      "Epoch 92/100\n",
      "997/997 [==============================] - 0s 145us/step - loss: 0.6937 - accuracy: 0.4935 - val_loss: 0.6926 - val_accuracy: 0.5676\n",
      "Epoch 93/100\n",
      "997/997 [==============================] - 0s 137us/step - loss: 0.6937 - accuracy: 0.4935 - val_loss: 0.6926 - val_accuracy: 0.5676\n",
      "Epoch 94/100\n",
      "997/997 [==============================] - 0s 146us/step - loss: 0.6937 - accuracy: 0.4935 - val_loss: 0.6926 - val_accuracy: 0.5766\n",
      "Epoch 95/100\n",
      "997/997 [==============================] - 0s 143us/step - loss: 0.6937 - accuracy: 0.4945 - val_loss: 0.6926 - val_accuracy: 0.5766\n",
      "Epoch 96/100\n",
      "997/997 [==============================] - 0s 144us/step - loss: 0.6937 - accuracy: 0.4945 - val_loss: 0.6926 - val_accuracy: 0.5766\n",
      "Epoch 97/100\n",
      "997/997 [==============================] - 0s 134us/step - loss: 0.6937 - accuracy: 0.4955 - val_loss: 0.6926 - val_accuracy: 0.5766\n",
      "Epoch 98/100\n",
      "997/997 [==============================] - 0s 143us/step - loss: 0.6937 - accuracy: 0.4975 - val_loss: 0.6926 - val_accuracy: 0.5766\n",
      "Epoch 99/100\n",
      "997/997 [==============================] - 0s 142us/step - loss: 0.6937 - accuracy: 0.4975 - val_loss: 0.6926 - val_accuracy: 0.5766\n",
      "Epoch 100/100\n",
      "997/997 [==============================] - 0s 136us/step - loss: 0.6937 - accuracy: 0.4995 - val_loss: 0.6925 - val_accuracy: 0.5766\n"
     ]
    }
   ],
   "source": [
    "history=classifier.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_split=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29d00267b8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training error and test error plots \n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy on training set = 49.950%\n",
      "Best Accuracy on validation set = 57.658%\n",
      "277/277 [==============================] - 0s 87us/step\n",
      "The loss on the test set is 0.6926 and the accuracy is 54.152%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Accuracy on training set = {max(history.history['accuracy'])*100:.3f}%\")\n",
    "print(f\"Best Accuracy on validation set = {max(history.history['val_accuracy'])*100:.3f}%\") \n",
    "test_loss, test_acc = classifier.evaluate(X_test, y_test['AdvancedFibrosis'])\n",
    "print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>model function</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=X.shape[1], activation='tanh'))\n",
    "    model.add(Dense(2, activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "epochs = 100\n",
    "batch_size = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1107/1107 [==============================] - 3s 2ms/step - loss: 0.7440 - accuracy: 0.4968\n",
      "Epoch 2/100\n",
      "1107/1107 [==============================] - 0s 237us/step - loss: 0.7300 - accuracy: 0.5023\n",
      "Epoch 3/100\n",
      "1107/1107 [==============================] - 0s 261us/step - loss: 0.7197 - accuracy: 0.4995\n",
      "Epoch 4/100\n",
      "1107/1107 [==============================] - 0s 240us/step - loss: 0.7119 - accuracy: 0.5041\n",
      "Epoch 5/100\n",
      "1107/1107 [==============================] - 0s 254us/step - loss: 0.7057 - accuracy: 0.5059\n",
      "Epoch 6/100\n",
      "1107/1107 [==============================] - 0s 259us/step - loss: 0.7009 - accuracy: 0.5104\n",
      "Epoch 7/100\n",
      "1107/1107 [==============================] - 0s 249us/step - loss: 0.6971 - accuracy: 0.5203\n",
      "Epoch 8/100\n",
      "1107/1107 [==============================] - 0s 236us/step - loss: 0.6939 - accuracy: 0.5266\n",
      "Epoch 9/100\n",
      "1107/1107 [==============================] - 0s 211us/step - loss: 0.6914 - accuracy: 0.5303\n",
      "Epoch 10/100\n",
      "1107/1107 [==============================] - 0s 224us/step - loss: 0.6893 - accuracy: 0.5339\n",
      "Epoch 11/100\n",
      "1107/1107 [==============================] - 0s 188us/step - loss: 0.6875 - accuracy: 0.5420\n",
      "Epoch 12/100\n",
      "1107/1107 [==============================] - 0s 202us/step - loss: 0.6859 - accuracy: 0.5510\n",
      "Epoch 13/100\n",
      "1107/1107 [==============================] - 0s 213us/step - loss: 0.6845 - accuracy: 0.56010s - loss: 0.6810 - accuracy: \n",
      "Epoch 14/100\n",
      "1107/1107 [==============================] - 0s 193us/step - loss: 0.6833 - accuracy: 0.5646\n",
      "Epoch 15/100\n",
      "1107/1107 [==============================] - 0s 187us/step - loss: 0.6822 - accuracy: 0.5610\n",
      "Epoch 16/100\n",
      "1107/1107 [==============================] - 0s 193us/step - loss: 0.6811 - accuracy: 0.5691\n",
      "Epoch 17/100\n",
      "1107/1107 [==============================] - 0s 178us/step - loss: 0.6801 - accuracy: 0.5745\n",
      "Epoch 18/100\n",
      "1107/1107 [==============================] - 0s 170us/step - loss: 0.6792 - accuracy: 0.5745\n",
      "Epoch 19/100\n",
      "1107/1107 [==============================] - 0s 162us/step - loss: 0.6782 - accuracy: 0.5790\n",
      "Epoch 20/100\n",
      "1107/1107 [==============================] - 0s 155us/step - loss: 0.6773 - accuracy: 0.5827\n",
      "Epoch 21/100\n",
      "1107/1107 [==============================] - 0s 168us/step - loss: 0.6765 - accuracy: 0.5836\n",
      "Epoch 22/100\n",
      "1107/1107 [==============================] - 0s 173us/step - loss: 0.6756 - accuracy: 0.5863\n",
      "Epoch 23/100\n",
      "1107/1107 [==============================] - 0s 162us/step - loss: 0.6747 - accuracy: 0.5863\n",
      "Epoch 24/100\n",
      "1107/1107 [==============================] - 0s 166us/step - loss: 0.6738 - accuracy: 0.5863\n",
      "Epoch 25/100\n",
      "1107/1107 [==============================] - 0s 164us/step - loss: 0.6730 - accuracy: 0.5899\n",
      "Epoch 26/100\n",
      "1107/1107 [==============================] - 0s 161us/step - loss: 0.6721 - accuracy: 0.5899\n",
      "Epoch 27/100\n",
      "1107/1107 [==============================] - 0s 158us/step - loss: 0.6712 - accuracy: 0.5926\n",
      "Epoch 28/100\n",
      "1107/1107 [==============================] - 0s 154us/step - loss: 0.6703 - accuracy: 0.5953\n",
      "Epoch 29/100\n",
      "1107/1107 [==============================] - 0s 155us/step - loss: 0.6694 - accuracy: 0.5998\n",
      "Epoch 30/100\n",
      "1107/1107 [==============================] - 0s 143us/step - loss: 0.6685 - accuracy: 0.6025\n",
      "Epoch 31/100\n",
      "1107/1107 [==============================] - 0s 136us/step - loss: 0.6676 - accuracy: 0.6016\n",
      "Epoch 32/100\n",
      "1107/1107 [==============================] - 0s 147us/step - loss: 0.6666 - accuracy: 0.6007\n",
      "Epoch 33/100\n",
      "1107/1107 [==============================] - 0s 153us/step - loss: 0.6656 - accuracy: 0.5989\n",
      "Epoch 34/100\n",
      "1107/1107 [==============================] - 0s 147us/step - loss: 0.6646 - accuracy: 0.5971\n",
      "Epoch 35/100\n",
      "1107/1107 [==============================] - 0s 140us/step - loss: 0.6636 - accuracy: 0.6052\n",
      "Epoch 36/100\n",
      "1107/1107 [==============================] - 0s 138us/step - loss: 0.6626 - accuracy: 0.6089\n",
      "Epoch 37/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6616 - accuracy: 0.6143\n",
      "Epoch 38/100\n",
      "1107/1107 [==============================] - 0s 164us/step - loss: 0.6605 - accuracy: 0.6125\n",
      "Epoch 39/100\n",
      "1107/1107 [==============================] - 0s 105us/step - loss: 0.6595 - accuracy: 0.6134\n",
      "Epoch 40/100\n",
      "1107/1107 [==============================] - 0s 108us/step - loss: 0.6584 - accuracy: 0.6170\n",
      "Epoch 41/100\n",
      "1107/1107 [==============================] - 0s 99us/step - loss: 0.6573 - accuracy: 0.6224\n",
      "Epoch 42/100\n",
      "1107/1107 [==============================] - 0s 115us/step - loss: 0.6563 - accuracy: 0.6242\n",
      "Epoch 43/100\n",
      "1107/1107 [==============================] - 0s 121us/step - loss: 0.6553 - accuracy: 0.6260\n",
      "Epoch 44/100\n",
      "1107/1107 [==============================] - 0s 123us/step - loss: 0.6542 - accuracy: 0.6269\n",
      "Epoch 45/100\n",
      "1107/1107 [==============================] - 0s 132us/step - loss: 0.6532 - accuracy: 0.6269\n",
      "Epoch 46/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6522 - accuracy: 0.6260\n",
      "Epoch 47/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6512 - accuracy: 0.6269\n",
      "Epoch 48/100\n",
      "1107/1107 [==============================] - 0s 122us/step - loss: 0.6502 - accuracy: 0.6314\n",
      "Epoch 49/100\n",
      "1107/1107 [==============================] - 0s 130us/step - loss: 0.6493 - accuracy: 0.6360\n",
      "Epoch 50/100\n",
      "1107/1107 [==============================] - 0s 175us/step - loss: 0.6483 - accuracy: 0.6369\n",
      "Epoch 51/100\n",
      "1107/1107 [==============================] - 0s 134us/step - loss: 0.6474 - accuracy: 0.6369\n",
      "Epoch 52/100\n",
      "1107/1107 [==============================] - 0s 119us/step - loss: 0.6465 - accuracy: 0.6369\n",
      "Epoch 53/100\n",
      "1107/1107 [==============================] - 0s 134us/step - loss: 0.6456 - accuracy: 0.6369\n",
      "Epoch 54/100\n",
      "1107/1107 [==============================] - 0s 183us/step - loss: 0.6447 - accuracy: 0.6387\n",
      "Epoch 55/100\n",
      "1107/1107 [==============================] - 0s 118us/step - loss: 0.6438 - accuracy: 0.6405\n",
      "Epoch 56/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6430 - accuracy: 0.6423\n",
      "Epoch 57/100\n",
      "1107/1107 [==============================] - 0s 133us/step - loss: 0.6421 - accuracy: 0.6432\n",
      "Epoch 58/100\n",
      "1107/1107 [==============================] - 0s 107us/step - loss: 0.6413 - accuracy: 0.6450\n",
      "Epoch 59/100\n",
      "1107/1107 [==============================] - 0s 125us/step - loss: 0.6404 - accuracy: 0.6450\n",
      "Epoch 60/100\n",
      "1107/1107 [==============================] - 0s 100us/step - loss: 0.6396 - accuracy: 0.6441\n",
      "Epoch 61/100\n",
      "1107/1107 [==============================] - 0s 113us/step - loss: 0.6388 - accuracy: 0.6459\n",
      "Epoch 62/100\n",
      "1107/1107 [==============================] - 0s 111us/step - loss: 0.6380 - accuracy: 0.6459\n",
      "Epoch 63/100\n",
      "1107/1107 [==============================] - 0s 99us/step - loss: 0.6373 - accuracy: 0.6477\n",
      "Epoch 64/100\n",
      "1107/1107 [==============================] - 0s 107us/step - loss: 0.6365 - accuracy: 0.6486\n",
      "Epoch 65/100\n",
      "1107/1107 [==============================] - 0s 109us/step - loss: 0.6357 - accuracy: 0.6495\n",
      "Epoch 66/100\n",
      "1107/1107 [==============================] - 0s 99us/step - loss: 0.6350 - accuracy: 0.6468\n",
      "Epoch 67/100\n",
      "1107/1107 [==============================] - 0s 100us/step - loss: 0.6343 - accuracy: 0.6486\n",
      "Epoch 68/100\n",
      "1107/1107 [==============================] - 0s 108us/step - loss: 0.6335 - accuracy: 0.6477\n",
      "Epoch 69/100\n",
      "1107/1107 [==============================] - 0s 105us/step - loss: 0.6328 - accuracy: 0.6477\n",
      "Epoch 70/100\n",
      "1107/1107 [==============================] - 0s 99us/step - loss: 0.6321 - accuracy: 0.6477\n",
      "Epoch 71/100\n",
      "1107/1107 [==============================] - 0s 103us/step - loss: 0.6314 - accuracy: 0.6495\n",
      "Epoch 72/100\n",
      "1107/1107 [==============================] - 0s 112us/step - loss: 0.6307 - accuracy: 0.6504\n",
      "Epoch 73/100\n",
      "1107/1107 [==============================] - 0s 101us/step - loss: 0.6301 - accuracy: 0.6486\n",
      "Epoch 74/100\n",
      "1107/1107 [==============================] - 0s 101us/step - loss: 0.6294 - accuracy: 0.6468\n",
      "Epoch 75/100\n",
      "1107/1107 [==============================] - 0s 111us/step - loss: 0.6287 - accuracy: 0.6477\n",
      "Epoch 76/100\n",
      "1107/1107 [==============================] - 0s 143us/step - loss: 0.6281 - accuracy: 0.6513\n",
      "Epoch 77/100\n",
      "1107/1107 [==============================] - 0s 111us/step - loss: 0.6274 - accuracy: 0.6531\n",
      "Epoch 78/100\n",
      "1107/1107 [==============================] - 0s 123us/step - loss: 0.6268 - accuracy: 0.6540\n",
      "Epoch 79/100\n",
      "1107/1107 [==============================] - 0s 124us/step - loss: 0.6262 - accuracy: 0.6576\n",
      "Epoch 80/100\n",
      "1107/1107 [==============================] - 0s 110us/step - loss: 0.6255 - accuracy: 0.6585\n",
      "Epoch 81/100\n",
      "1107/1107 [==============================] - 0s 104us/step - loss: 0.6249 - accuracy: 0.6585\n",
      "Epoch 82/100\n",
      "1107/1107 [==============================] - 0s 106us/step - loss: 0.6243 - accuracy: 0.6594\n",
      "Epoch 83/100\n",
      "1107/1107 [==============================] - 0s 103us/step - loss: 0.6237 - accuracy: 0.6612\n",
      "Epoch 84/100\n",
      "1107/1107 [==============================] - 0s 123us/step - loss: 0.6231 - accuracy: 0.6612\n",
      "Epoch 85/100\n",
      "1107/1107 [==============================] - 0s 141us/step - loss: 0.6226 - accuracy: 0.6621\n",
      "Epoch 86/100\n",
      "1107/1107 [==============================] - 0s 165us/step - loss: 0.6220 - accuracy: 0.6631\n",
      "Epoch 87/100\n",
      "1107/1107 [==============================] - 0s 147us/step - loss: 0.6214 - accuracy: 0.6640\n",
      "Epoch 88/100\n",
      "1107/1107 [==============================] - 0s 151us/step - loss: 0.6209 - accuracy: 0.6631\n",
      "Epoch 89/100\n",
      "1107/1107 [==============================] - 0s 138us/step - loss: 0.6203 - accuracy: 0.6603\n",
      "Epoch 90/100\n",
      "1107/1107 [==============================] - 0s 109us/step - loss: 0.6198 - accuracy: 0.6612\n",
      "Epoch 91/100\n",
      "1107/1107 [==============================] - 0s 100us/step - loss: 0.6193 - accuracy: 0.6649\n",
      "Epoch 92/100\n",
      "1107/1107 [==============================] - 0s 100us/step - loss: 0.6187 - accuracy: 0.6649\n",
      "Epoch 93/100\n",
      "1107/1107 [==============================] - 0s 114us/step - loss: 0.6182 - accuracy: 0.6658\n",
      "Epoch 94/100\n",
      "1107/1107 [==============================] - 0s 106us/step - loss: 0.6177 - accuracy: 0.6676\n",
      "Epoch 95/100\n",
      "1107/1107 [==============================] - 0s 100us/step - loss: 0.6172 - accuracy: 0.6667\n",
      "Epoch 96/100\n",
      "1107/1107 [==============================] - 0s 105us/step - loss: 0.6167 - accuracy: 0.6694\n",
      "Epoch 97/100\n",
      "1107/1107 [==============================] - 0s 109us/step - loss: 0.6163 - accuracy: 0.6712\n",
      "Epoch 98/100\n",
      "1107/1107 [==============================] - 0s 107us/step - loss: 0.6158 - accuracy: 0.6730\n",
      "Epoch 99/100\n",
      "1107/1107 [==============================] - 0s 102us/step - loss: 0.6153 - accuracy: 0.6739\n",
      "Epoch 100/100\n",
      "1107/1107 [==============================] - 0s 102us/step - loss: 0.6149 - accuracy: 0.6739\n",
      "278/278 [==============================] - 0s 998us/step\n",
      "Epoch 1/100\n",
      "1107/1107 [==============================] - 2s 2ms/step - loss: 0.7337 - accuracy: 0.4860\n",
      "Epoch 2/100\n",
      "1107/1107 [==============================] - 0s 221us/step - loss: 0.7212 - accuracy: 0.4842\n",
      "Epoch 3/100\n",
      "1107/1107 [==============================] - 0s 239us/step - loss: 0.7123 - accuracy: 0.4968\n",
      "Epoch 4/100\n",
      "1107/1107 [==============================] - 0s 236us/step - loss: 0.7056 - accuracy: 0.5077\n",
      "Epoch 5/100\n",
      "1107/1107 [==============================] - 0s 217us/step - loss: 0.7003 - accuracy: 0.5149\n",
      "Epoch 6/100\n",
      "1107/1107 [==============================] - 0s 212us/step - loss: 0.6960 - accuracy: 0.5221\n",
      "Epoch 7/100\n",
      "1107/1107 [==============================] - 0s 185us/step - loss: 0.6924 - accuracy: 0.5366\n",
      "Epoch 8/100\n",
      "1107/1107 [==============================] - 0s 213us/step - loss: 0.6894 - accuracy: 0.5384\n",
      "Epoch 9/100\n",
      "1107/1107 [==============================] - 0s 184us/step - loss: 0.6866 - accuracy: 0.5501\n",
      "Epoch 10/100\n",
      "1107/1107 [==============================] - 0s 190us/step - loss: 0.6842 - accuracy: 0.5556\n",
      "Epoch 11/100\n",
      "1107/1107 [==============================] - 0s 172us/step - loss: 0.6819 - accuracy: 0.5601\n",
      "Epoch 12/100\n",
      "1107/1107 [==============================] - 0s 250us/step - loss: 0.6797 - accuracy: 0.5637\n",
      "Epoch 13/100\n",
      "1107/1107 [==============================] - 0s 215us/step - loss: 0.6776 - accuracy: 0.5610\n",
      "Epoch 14/100\n",
      "1107/1107 [==============================] - 0s 201us/step - loss: 0.6756 - accuracy: 0.5718\n",
      "Epoch 15/100\n",
      "1107/1107 [==============================] - 0s 206us/step - loss: 0.6736 - accuracy: 0.5781\n",
      "Epoch 16/100\n",
      "1107/1107 [==============================] - 0s 177us/step - loss: 0.6719 - accuracy: 0.5836\n",
      "Epoch 17/100\n",
      "1107/1107 [==============================] - 0s 168us/step - loss: 0.6703 - accuracy: 0.5881\n",
      "Epoch 18/100\n",
      "1107/1107 [==============================] - 0s 173us/step - loss: 0.6689 - accuracy: 0.5935\n",
      "Epoch 19/100\n",
      "1107/1107 [==============================] - 0s 157us/step - loss: 0.6676 - accuracy: 0.5890\n",
      "Epoch 20/100\n",
      "1107/1107 [==============================] - 0s 146us/step - loss: 0.6664 - accuracy: 0.5917\n",
      "Epoch 21/100\n",
      "1107/1107 [==============================] - 0s 140us/step - loss: 0.6653 - accuracy: 0.5953\n",
      "Epoch 22/100\n",
      "1107/1107 [==============================] - 0s 134us/step - loss: 0.6642 - accuracy: 0.5944\n",
      "Epoch 23/100\n",
      "1107/1107 [==============================] - 0s 135us/step - loss: 0.6632 - accuracy: 0.5935\n",
      "Epoch 24/100\n",
      "1107/1107 [==============================] - 0s 129us/step - loss: 0.6621 - accuracy: 0.5944\n",
      "Epoch 25/100\n",
      "1107/1107 [==============================] - 0s 128us/step - loss: 0.6611 - accuracy: 0.5962\n",
      "Epoch 26/100\n",
      "1107/1107 [==============================] - 0s 138us/step - loss: 0.6602 - accuracy: 0.5980\n",
      "Epoch 27/100\n",
      "1107/1107 [==============================] - 0s 135us/step - loss: 0.6592 - accuracy: 0.5998\n",
      "Epoch 28/100\n",
      "1107/1107 [==============================] - 0s 136us/step - loss: 0.6583 - accuracy: 0.5998\n",
      "Epoch 29/100\n",
      "1107/1107 [==============================] - 0s 133us/step - loss: 0.6573 - accuracy: 0.6016\n",
      "Epoch 30/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6564 - accuracy: 0.6025\n",
      "Epoch 31/100\n",
      "1107/1107 [==============================] - 0s 125us/step - loss: 0.6555 - accuracy: 0.6016\n",
      "Epoch 32/100\n",
      "1107/1107 [==============================] - 0s 126us/step - loss: 0.6546 - accuracy: 0.6016\n",
      "Epoch 33/100\n",
      "1107/1107 [==============================] - 0s 123us/step - loss: 0.6537 - accuracy: 0.5998\n",
      "Epoch 34/100\n",
      "1107/1107 [==============================] - 0s 131us/step - loss: 0.6529 - accuracy: 0.5989\n",
      "Epoch 35/100\n",
      "1107/1107 [==============================] - 0s 185us/step - loss: 0.6520 - accuracy: 0.5989\n",
      "Epoch 36/100\n",
      "1107/1107 [==============================] - 0s 135us/step - loss: 0.6512 - accuracy: 0.5980\n",
      "Epoch 37/100\n",
      "1107/1107 [==============================] - 0s 124us/step - loss: 0.6504 - accuracy: 0.6007\n",
      "Epoch 38/100\n",
      "1107/1107 [==============================] - 0s 122us/step - loss: 0.6496 - accuracy: 0.6007\n",
      "Epoch 39/100\n",
      "1107/1107 [==============================] - 0s 161us/step - loss: 0.6488 - accuracy: 0.6016\n",
      "Epoch 40/100\n",
      "1107/1107 [==============================] - 0s 165us/step - loss: 0.6481 - accuracy: 0.6025\n",
      "Epoch 41/100\n",
      "1107/1107 [==============================] - 0s 175us/step - loss: 0.6473 - accuracy: 0.6025\n",
      "Epoch 42/100\n",
      "1107/1107 [==============================] - 0s 159us/step - loss: 0.6466 - accuracy: 0.6016\n",
      "Epoch 43/100\n",
      "1107/1107 [==============================] - 0s 141us/step - loss: 0.6460 - accuracy: 0.6034\n",
      "Epoch 44/100\n",
      "1107/1107 [==============================] - 0s 132us/step - loss: 0.6453 - accuracy: 0.6043\n",
      "Epoch 45/100\n",
      "1107/1107 [==============================] - 0s 125us/step - loss: 0.6447 - accuracy: 0.6043\n",
      "Epoch 46/100\n",
      "1107/1107 [==============================] - 0s 121us/step - loss: 0.6440 - accuracy: 0.6043\n",
      "Epoch 47/100\n",
      "1107/1107 [==============================] - 0s 119us/step - loss: 0.6434 - accuracy: 0.6070\n",
      "Epoch 48/100\n",
      "1107/1107 [==============================] - 0s 122us/step - loss: 0.6428 - accuracy: 0.6098\n",
      "Epoch 49/100\n",
      "1107/1107 [==============================] - 0s 122us/step - loss: 0.6422 - accuracy: 0.6116\n",
      "Epoch 50/100\n",
      "1107/1107 [==============================] - 0s 118us/step - loss: 0.6416 - accuracy: 0.6116\n",
      "Epoch 51/100\n",
      "1107/1107 [==============================] - 0s 120us/step - loss: 0.6410 - accuracy: 0.6125\n",
      "Epoch 52/100\n",
      "1107/1107 [==============================] - 0s 113us/step - loss: 0.6404 - accuracy: 0.6143\n",
      "Epoch 53/100\n",
      "1107/1107 [==============================] - 0s 118us/step - loss: 0.6398 - accuracy: 0.6152\n",
      "Epoch 54/100\n",
      "1107/1107 [==============================] - 0s 115us/step - loss: 0.6393 - accuracy: 0.6152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6387 - accuracy: 0.6152\n",
      "Epoch 56/100\n",
      "1107/1107 [==============================] - 0s 146us/step - loss: 0.6381 - accuracy: 0.6152\n",
      "Epoch 57/100\n",
      "1107/1107 [==============================] - 0s 134us/step - loss: 0.6376 - accuracy: 0.6134\n",
      "Epoch 58/100\n",
      "1107/1107 [==============================] - 0s 140us/step - loss: 0.6370 - accuracy: 0.6152\n",
      "Epoch 59/100\n",
      "1107/1107 [==============================] - 0s 122us/step - loss: 0.6365 - accuracy: 0.6161\n",
      "Epoch 60/100\n",
      "1107/1107 [==============================] - 0s 132us/step - loss: 0.6359 - accuracy: 0.6161\n",
      "Epoch 61/100\n",
      "1107/1107 [==============================] - 0s 112us/step - loss: 0.6354 - accuracy: 0.6152\n",
      "Epoch 62/100\n",
      "1107/1107 [==============================] - 0s 122us/step - loss: 0.6349 - accuracy: 0.6143\n",
      "Epoch 63/100\n",
      "1107/1107 [==============================] - 0s 126us/step - loss: 0.6343 - accuracy: 0.6170\n",
      "Epoch 64/100\n",
      "1107/1107 [==============================] - 0s 116us/step - loss: 0.6338 - accuracy: 0.6179\n",
      "Epoch 65/100\n",
      "1107/1107 [==============================] - 0s 126us/step - loss: 0.6333 - accuracy: 0.6170\n",
      "Epoch 66/100\n",
      "1107/1107 [==============================] - 0s 132us/step - loss: 0.6328 - accuracy: 0.6179\n",
      "Epoch 67/100\n",
      "1107/1107 [==============================] - 0s 125us/step - loss: 0.6323 - accuracy: 0.6170\n",
      "Epoch 68/100\n",
      "1107/1107 [==============================] - 0s 120us/step - loss: 0.6318 - accuracy: 0.6179\n",
      "Epoch 69/100\n",
      "1107/1107 [==============================] - 0s 120us/step - loss: 0.6313 - accuracy: 0.6188\n",
      "Epoch 70/100\n",
      "1107/1107 [==============================] - 0s 118us/step - loss: 0.6308 - accuracy: 0.6197\n",
      "Epoch 71/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6303 - accuracy: 0.6215\n",
      "Epoch 72/100\n",
      "1107/1107 [==============================] - 0s 118us/step - loss: 0.6298 - accuracy: 0.6224\n",
      "Epoch 73/100\n",
      "1107/1107 [==============================] - 0s 123us/step - loss: 0.6293 - accuracy: 0.6233\n",
      "Epoch 74/100\n",
      "1107/1107 [==============================] - 0s 115us/step - loss: 0.6289 - accuracy: 0.6233\n",
      "Epoch 75/100\n",
      "1107/1107 [==============================] - 0s 108us/step - loss: 0.6284 - accuracy: 0.6242\n",
      "Epoch 76/100\n",
      "1107/1107 [==============================] - 0s 121us/step - loss: 0.6279 - accuracy: 0.6233\n",
      "Epoch 77/100\n",
      "1107/1107 [==============================] - 0s 125us/step - loss: 0.6274 - accuracy: 0.6233\n",
      "Epoch 78/100\n",
      "1107/1107 [==============================] - 0s 151us/step - loss: 0.6269 - accuracy: 0.6260\n",
      "Epoch 79/100\n",
      "1107/1107 [==============================] - 0s 136us/step - loss: 0.6264 - accuracy: 0.6260\n",
      "Epoch 80/100\n",
      "1107/1107 [==============================] - 0s 129us/step - loss: 0.6259 - accuracy: 0.6251\n",
      "Epoch 81/100\n",
      "1107/1107 [==============================] - 0s 126us/step - loss: 0.6255 - accuracy: 0.6251\n",
      "Epoch 82/100\n",
      "1107/1107 [==============================] - 0s 129us/step - loss: 0.6250 - accuracy: 0.6287\n",
      "Epoch 83/100\n",
      "1107/1107 [==============================] - 0s 122us/step - loss: 0.6245 - accuracy: 0.6251\n",
      "Epoch 84/100\n",
      "1107/1107 [==============================] - 0s 130us/step - loss: 0.6239 - accuracy: 0.6269\n",
      "Epoch 85/100\n",
      "1107/1107 [==============================] - 0s 115us/step - loss: 0.6234 - accuracy: 0.6278\n",
      "Epoch 86/100\n",
      "1107/1107 [==============================] - 0s 120us/step - loss: 0.6229 - accuracy: 0.6305\n",
      "Epoch 87/100\n",
      "1107/1107 [==============================] - 0s 125us/step - loss: 0.6224 - accuracy: 0.6287\n",
      "Epoch 88/100\n",
      "1107/1107 [==============================] - 0s 118us/step - loss: 0.6218 - accuracy: 0.6278\n",
      "Epoch 89/100\n",
      "1107/1107 [==============================] - 0s 118us/step - loss: 0.6213 - accuracy: 0.6260\n",
      "Epoch 90/100\n",
      "1107/1107 [==============================] - 0s 126us/step - loss: 0.6208 - accuracy: 0.6269\n",
      "Epoch 91/100\n",
      "1107/1107 [==============================] - 0s 114us/step - loss: 0.6202 - accuracy: 0.6251\n",
      "Epoch 92/100\n",
      "1107/1107 [==============================] - 0s 121us/step - loss: 0.6197 - accuracy: 0.6260\n",
      "Epoch 93/100\n",
      "1107/1107 [==============================] - 0s 119us/step - loss: 0.6192 - accuracy: 0.6278\n",
      "Epoch 94/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6187 - accuracy: 0.6287\n",
      "Epoch 95/100\n",
      "1107/1107 [==============================] - 0s 121us/step - loss: 0.6182 - accuracy: 0.6287\n",
      "Epoch 96/100\n",
      "1107/1107 [==============================] - 0s 127us/step - loss: 0.6177 - accuracy: 0.6269\n",
      "Epoch 97/100\n",
      "1107/1107 [==============================] - 0s 120us/step - loss: 0.6172 - accuracy: 0.6251\n",
      "Epoch 98/100\n",
      "1107/1107 [==============================] - 0s 124us/step - loss: 0.6167 - accuracy: 0.6260\n",
      "Epoch 99/100\n",
      "1107/1107 [==============================] - 0s 128us/step - loss: 0.6162 - accuracy: 0.6278\n",
      "Epoch 100/100\n",
      "1107/1107 [==============================] - 0s 131us/step - loss: 0.6157 - accuracy: 0.6278\n",
      "278/278 [==============================] - 0s 743us/step\n",
      "Epoch 1/100\n",
      "1108/1108 [==============================] - 2s 2ms/step - loss: 0.7161 - accuracy: 0.5253\n",
      "Epoch 2/100\n",
      "1108/1108 [==============================] - 0s 252us/step - loss: 0.7081 - accuracy: 0.52980s - loss: 0.7063 - accura\n",
      "Epoch 3/100\n",
      "1108/1108 [==============================] - 0s 235us/step - loss: 0.7026 - accuracy: 0.5352\n",
      "Epoch 4/100\n",
      "1108/1108 [==============================] - 0s 238us/step - loss: 0.6984 - accuracy: 0.5325\n",
      "Epoch 5/100\n",
      "1108/1108 [==============================] - 0s 243us/step - loss: 0.6951 - accuracy: 0.5433\n",
      "Epoch 6/100\n",
      "1108/1108 [==============================] - 0s 258us/step - loss: 0.6924 - accuracy: 0.5451\n",
      "Epoch 7/100\n",
      "1108/1108 [==============================] - 0s 256us/step - loss: 0.6901 - accuracy: 0.5523\n",
      "Epoch 8/100\n",
      "1108/1108 [==============================] - 0s 237us/step - loss: 0.6882 - accuracy: 0.5551\n",
      "Epoch 9/100\n",
      "1108/1108 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - 0s 216us/step - loss: 0.6865 - accuracy: 0.5551\n",
      "Epoch 10/100\n",
      "1108/1108 [==============================] - 0s 216us/step - loss: 0.6850 - accuracy: 0.5587\n",
      "Epoch 11/100\n",
      "1108/1108 [==============================] - 0s 204us/step - loss: 0.6837 - accuracy: 0.5650\n",
      "Epoch 12/100\n",
      "1108/1108 [==============================] - 0s 203us/step - loss: 0.6825 - accuracy: 0.5704\n",
      "Epoch 13/100\n",
      "1108/1108 [==============================] - 0s 193us/step - loss: 0.6813 - accuracy: 0.57400s - loss: 0.6844 - accuracy\n",
      "Epoch 14/100\n",
      "1108/1108 [==============================] - 0s 180us/step - loss: 0.6802 - accuracy: 0.5713\n",
      "Epoch 15/100\n",
      "1108/1108 [==============================] - 0s 187us/step - loss: 0.6791 - accuracy: 0.5740\n",
      "Epoch 16/100\n",
      "1108/1108 [==============================] - 0s 176us/step - loss: 0.6781 - accuracy: 0.5740\n",
      "Epoch 17/100\n",
      "1108/1108 [==============================] - 0s 170us/step - loss: 0.6770 - accuracy: 0.5794\n",
      "Epoch 18/100\n",
      "1108/1108 [==============================] - 0s 167us/step - loss: 0.6760 - accuracy: 0.5740\n",
      "Epoch 19/100\n",
      "1108/1108 [==============================] - 0s 157us/step - loss: 0.6750 - accuracy: 0.5776\n",
      "Epoch 20/100\n",
      "1108/1108 [==============================] - 0s 160us/step - loss: 0.6740 - accuracy: 0.5821\n",
      "Epoch 21/100\n",
      "1108/1108 [==============================] - 0s 172us/step - loss: 0.6729 - accuracy: 0.58300s - loss: 0.6725 - accuracy: \n",
      "Epoch 22/100\n",
      "1108/1108 [==============================] - 0s 173us/step - loss: 0.6719 - accuracy: 0.5866\n",
      "Epoch 23/100\n",
      "1108/1108 [==============================] - 0s 161us/step - loss: 0.6708 - accuracy: 0.5848\n",
      "Epoch 24/100\n",
      "1108/1108 [==============================] - 0s 163us/step - loss: 0.6698 - accuracy: 0.5912\n",
      "Epoch 25/100\n",
      "1108/1108 [==============================] - 0s 164us/step - loss: 0.6687 - accuracy: 0.5939\n",
      "Epoch 26/100\n",
      "1108/1108 [==============================] - 0s 143us/step - loss: 0.6677 - accuracy: 0.5957\n",
      "Epoch 27/100\n",
      "1108/1108 [==============================] - 0s 154us/step - loss: 0.6667 - accuracy: 0.6011\n",
      "Epoch 28/100\n",
      "1108/1108 [==============================] - 0s 152us/step - loss: 0.6656 - accuracy: 0.6038\n",
      "Epoch 29/100\n",
      "1108/1108 [==============================] - 0s 139us/step - loss: 0.6647 - accuracy: 0.6011\n",
      "Epoch 30/100\n",
      "1108/1108 [==============================] - 0s 140us/step - loss: 0.6637 - accuracy: 0.5975\n",
      "Epoch 31/100\n",
      "1108/1108 [==============================] - 0s 137us/step - loss: 0.6628 - accuracy: 0.6047\n",
      "Epoch 32/100\n",
      "1108/1108 [==============================] - 0s 145us/step - loss: 0.6619 - accuracy: 0.6074\n",
      "Epoch 33/100\n",
      "1108/1108 [==============================] - 0s 132us/step - loss: 0.6610 - accuracy: 0.6083\n",
      "Epoch 34/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6601 - accuracy: 0.6101\n",
      "Epoch 35/100\n",
      "1108/1108 [==============================] - 0s 132us/step - loss: 0.6593 - accuracy: 0.6119\n",
      "Epoch 36/100\n",
      "1108/1108 [==============================] - 0s 135us/step - loss: 0.6584 - accuracy: 0.6128\n",
      "Epoch 37/100\n",
      "1108/1108 [==============================] - 0s 147us/step - loss: 0.6576 - accuracy: 0.6128\n",
      "Epoch 38/100\n",
      "1108/1108 [==============================] - 0s 164us/step - loss: 0.6568 - accuracy: 0.6155\n",
      "Epoch 39/100\n",
      "1108/1108 [==============================] - 0s 145us/step - loss: 0.6561 - accuracy: 0.6173\n",
      "Epoch 40/100\n",
      "1108/1108 [==============================] - 0s 127us/step - loss: 0.6553 - accuracy: 0.6191\n",
      "Epoch 41/100\n",
      "1108/1108 [==============================] - 0s 122us/step - loss: 0.6546 - accuracy: 0.6236\n",
      "Epoch 42/100\n",
      "1108/1108 [==============================] - 0s 120us/step - loss: 0.6538 - accuracy: 0.6236\n",
      "Epoch 43/100\n",
      "1108/1108 [==============================] - 0s 120us/step - loss: 0.6531 - accuracy: 0.6264\n",
      "Epoch 44/100\n",
      "1108/1108 [==============================] - 0s 119us/step - loss: 0.6524 - accuracy: 0.6264\n",
      "Epoch 45/100\n",
      "1108/1108 [==============================] - 0s 120us/step - loss: 0.6517 - accuracy: 0.6236\n",
      "Epoch 46/100\n",
      "1108/1108 [==============================] - 0s 125us/step - loss: 0.6510 - accuracy: 0.6245\n",
      "Epoch 47/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6503 - accuracy: 0.6255\n",
      "Epoch 48/100\n",
      "1108/1108 [==============================] - 0s 118us/step - loss: 0.6496 - accuracy: 0.6273\n",
      "Epoch 49/100\n",
      "1108/1108 [==============================] - 0s 117us/step - loss: 0.6490 - accuracy: 0.6300\n",
      "Epoch 50/100\n",
      "1108/1108 [==============================] - 0s 118us/step - loss: 0.6483 - accuracy: 0.6300\n",
      "Epoch 51/100\n",
      "1108/1108 [==============================] - 0s 120us/step - loss: 0.6477 - accuracy: 0.6300\n",
      "Epoch 52/100\n",
      "1108/1108 [==============================] - 0s 123us/step - loss: 0.6470 - accuracy: 0.6327\n",
      "Epoch 53/100\n",
      "1108/1108 [==============================] - 0s 121us/step - loss: 0.6464 - accuracy: 0.6363\n",
      "Epoch 54/100\n",
      "1108/1108 [==============================] - 0s 118us/step - loss: 0.6458 - accuracy: 0.6372\n",
      "Epoch 55/100\n",
      "1108/1108 [==============================] - 0s 114us/step - loss: 0.6452 - accuracy: 0.6372\n",
      "Epoch 56/100\n",
      "1108/1108 [==============================] - 0s 115us/step - loss: 0.6446 - accuracy: 0.6363\n",
      "Epoch 57/100\n",
      "1108/1108 [==============================] - 0s 116us/step - loss: 0.6440 - accuracy: 0.6354\n",
      "Epoch 58/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6434 - accuracy: 0.6354\n",
      "Epoch 59/100\n",
      "1108/1108 [==============================] - 0s 125us/step - loss: 0.6428 - accuracy: 0.6354\n",
      "Epoch 60/100\n",
      "1108/1108 [==============================] - 0s 125us/step - loss: 0.6422 - accuracy: 0.6363\n",
      "Epoch 61/100\n",
      "1108/1108 [==============================] - 0s 123us/step - loss: 0.6416 - accuracy: 0.6381\n",
      "Epoch 62/100\n",
      "1108/1108 [==============================] - 0s 150us/step - loss: 0.6410 - accuracy: 0.6408\n",
      "Epoch 63/100\n",
      "1108/1108 [==============================] - 0s 119us/step - loss: 0.6404 - accuracy: 0.6435\n",
      "Epoch 64/100\n",
      "1108/1108 [==============================] - 0s 119us/step - loss: 0.6398 - accuracy: 0.6453\n",
      "Epoch 65/100\n",
      "1108/1108 [==============================] - 0s 127us/step - loss: 0.6393 - accuracy: 0.6453\n",
      "Epoch 66/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6387 - accuracy: 0.6444\n",
      "Epoch 67/100\n",
      "1108/1108 [==============================] - 0s 116us/step - loss: 0.6381 - accuracy: 0.6462\n",
      "Epoch 68/100\n",
      "1108/1108 [==============================] - 0s 123us/step - loss: 0.6375 - accuracy: 0.6471\n",
      "Epoch 69/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6369 - accuracy: 0.6471\n",
      "Epoch 70/100\n",
      "1108/1108 [==============================] - 0s 119us/step - loss: 0.6364 - accuracy: 0.6453\n",
      "Epoch 71/100\n",
      "1108/1108 [==============================] - 0s 122us/step - loss: 0.6358 - accuracy: 0.6462\n",
      "Epoch 72/100\n",
      "1108/1108 [==============================] - 0s 123us/step - loss: 0.6352 - accuracy: 0.6435\n",
      "Epoch 73/100\n",
      "1108/1108 [==============================] - 0s 130us/step - loss: 0.6346 - accuracy: 0.6417\n",
      "Epoch 74/100\n",
      "1108/1108 [==============================] - 0s 131us/step - loss: 0.6340 - accuracy: 0.6453\n",
      "Epoch 75/100\n",
      "1108/1108 [==============================] - 0s 131us/step - loss: 0.6334 - accuracy: 0.6453\n",
      "Epoch 76/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6328 - accuracy: 0.6444\n",
      "Epoch 77/100\n",
      "1108/1108 [==============================] - 0s 109us/step - loss: 0.6321 - accuracy: 0.6426\n",
      "Epoch 78/100\n",
      "1108/1108 [==============================] - 0s 120us/step - loss: 0.6315 - accuracy: 0.6444\n",
      "Epoch 79/100\n",
      "1108/1108 [==============================] - 0s 119us/step - loss: 0.6309 - accuracy: 0.6453\n",
      "Epoch 80/100\n",
      "1108/1108 [==============================] - 0s 113us/step - loss: 0.6302 - accuracy: 0.6453\n",
      "Epoch 81/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6296 - accuracy: 0.6444\n",
      "Epoch 82/100\n",
      "1108/1108 [==============================] - 0s 160us/step - loss: 0.6289 - accuracy: 0.6462\n",
      "Epoch 83/100\n",
      "1108/1108 [==============================] - 0s 131us/step - loss: 0.6282 - accuracy: 0.6489\n",
      "Epoch 84/100\n",
      "1108/1108 [==============================] - 0s 128us/step - loss: 0.6276 - accuracy: 0.6480\n",
      "Epoch 85/100\n",
      "1108/1108 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.65 - 0s 161us/step - loss: 0.6269 - accuracy: 0.6498\n",
      "Epoch 86/100\n",
      "1108/1108 [==============================] - 0s 174us/step - loss: 0.6262 - accuracy: 0.6507\n",
      "Epoch 87/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6255 - accuracy: 0.6507\n",
      "Epoch 88/100\n",
      "1108/1108 [==============================] - 0s 100us/step - loss: 0.6248 - accuracy: 0.6525\n",
      "Epoch 89/100\n",
      "1108/1108 [==============================] - 0s 102us/step - loss: 0.6241 - accuracy: 0.6507\n",
      "Epoch 90/100\n",
      "1108/1108 [==============================] - 0s 101us/step - loss: 0.6233 - accuracy: 0.6462\n",
      "Epoch 91/100\n",
      "1108/1108 [==============================] - 0s 98us/step - loss: 0.6226 - accuracy: 0.6453\n",
      "Epoch 92/100\n",
      "1108/1108 [==============================] - 0s 113us/step - loss: 0.6219 - accuracy: 0.6471\n",
      "Epoch 93/100\n",
      "1108/1108 [==============================] - 0s 154us/step - loss: 0.6212 - accuracy: 0.6489\n",
      "Epoch 94/100\n",
      "1108/1108 [==============================] - 0s 126us/step - loss: 0.6204 - accuracy: 0.6516\n",
      "Epoch 95/100\n",
      "1108/1108 [==============================] - 0s 107us/step - loss: 0.6197 - accuracy: 0.6534\n",
      "Epoch 96/100\n",
      "1108/1108 [==============================] - 0s 98us/step - loss: 0.6189 - accuracy: 0.6534\n",
      "Epoch 97/100\n",
      "1108/1108 [==============================] - 0s 110us/step - loss: 0.6182 - accuracy: 0.6534\n",
      "Epoch 98/100\n",
      "1108/1108 [==============================] - 0s 109us/step - loss: 0.6174 - accuracy: 0.6534\n",
      "Epoch 99/100\n",
      "1108/1108 [==============================] - 0s 98us/step - loss: 0.6166 - accuracy: 0.6543\n",
      "Epoch 100/100\n",
      "1108/1108 [==============================] - 0s 104us/step - loss: 0.6159 - accuracy: 0.6534\n",
      "277/277 [==============================] - 0s 647us/step\n",
      "Epoch 1/100\n",
      "1109/1109 [==============================] - 2s 2ms/step - loss: 0.8032 - accuracy: 0.5203\n",
      "Epoch 2/100\n",
      "1109/1109 [==============================] - 0s 210us/step - loss: 0.7696 - accuracy: 0.5311\n",
      "Epoch 3/100\n",
      "1109/1109 [==============================] - 0s 206us/step - loss: 0.7451 - accuracy: 0.5374\n",
      "Epoch 4/100\n",
      "1109/1109 [==============================] - 0s 210us/step - loss: 0.7273 - accuracy: 0.5410\n",
      "Epoch 5/100\n",
      "1109/1109 [==============================] - 0s 202us/step - loss: 0.7144 - accuracy: 0.5392\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109/1109 [==============================] - 0s 208us/step - loss: 0.7051 - accuracy: 0.5428\n",
      "Epoch 7/100\n",
      "1109/1109 [==============================] - 0s 182us/step - loss: 0.6983 - accuracy: 0.5455\n",
      "Epoch 8/100\n",
      "1109/1109 [==============================] - 0s 183us/step - loss: 0.6933 - accuracy: 0.5546\n",
      "Epoch 9/100\n",
      "1109/1109 [==============================] - 0s 180us/step - loss: 0.6893 - accuracy: 0.5627\n",
      "Epoch 10/100\n",
      "1109/1109 [==============================] - 0s 180us/step - loss: 0.6863 - accuracy: 0.5627\n",
      "Epoch 11/100\n",
      "1109/1109 [==============================] - 0s 180us/step - loss: 0.6838 - accuracy: 0.5645\n",
      "Epoch 12/100\n",
      "1109/1109 [==============================] - 0s 183us/step - loss: 0.6817 - accuracy: 0.5591\n",
      "Epoch 13/100\n",
      "1109/1109 [==============================] - 0s 183us/step - loss: 0.6799 - accuracy: 0.5618\n",
      "Epoch 14/100\n",
      "1109/1109 [==============================] - 0s 182us/step - loss: 0.6784 - accuracy: 0.5591\n",
      "Epoch 15/100\n",
      "1109/1109 [==============================] - 0s 180us/step - loss: 0.6771 - accuracy: 0.5591\n",
      "Epoch 16/100\n",
      "1109/1109 [==============================] - 0s 189us/step - loss: 0.6758 - accuracy: 0.5609\n",
      "Epoch 17/100\n",
      "1109/1109 [==============================] - 0s 180us/step - loss: 0.6747 - accuracy: 0.5654\n",
      "Epoch 18/100\n",
      "1109/1109 [==============================] - 0s 191us/step - loss: 0.6737 - accuracy: 0.5645\n",
      "Epoch 19/100\n",
      "1109/1109 [==============================] - 0s 187us/step - loss: 0.6727 - accuracy: 0.5663\n",
      "Epoch 20/100\n",
      "1109/1109 [==============================] - 0s 184us/step - loss: 0.6718 - accuracy: 0.5735\n",
      "Epoch 21/100\n",
      "1109/1109 [==============================] - 0s 181us/step - loss: 0.6709 - accuracy: 0.5735\n",
      "Epoch 22/100\n",
      "1109/1109 [==============================] - 0s 186us/step - loss: 0.6701 - accuracy: 0.5753\n",
      "Epoch 23/100\n",
      "1109/1109 [==============================] - 0s 185us/step - loss: 0.6693 - accuracy: 0.5726\n",
      "Epoch 24/100\n",
      "1109/1109 [==============================] - 0s 179us/step - loss: 0.6685 - accuracy: 0.5726\n",
      "Epoch 25/100\n",
      "1109/1109 [==============================] - 0s 176us/step - loss: 0.6678 - accuracy: 0.5744\n",
      "Epoch 26/100\n",
      "1109/1109 [==============================] - 0s 164us/step - loss: 0.6671 - accuracy: 0.5744\n",
      "Epoch 27/100\n",
      "1109/1109 [==============================] - 0s 166us/step - loss: 0.6664 - accuracy: 0.5771\n",
      "Epoch 28/100\n",
      "1109/1109 [==============================] - 0s 159us/step - loss: 0.6657 - accuracy: 0.5780\n",
      "Epoch 29/100\n",
      "1109/1109 [==============================] - 0s 151us/step - loss: 0.6650 - accuracy: 0.5789\n",
      "Epoch 30/100\n",
      "1109/1109 [==============================] - 0s 152us/step - loss: 0.6643 - accuracy: 0.5789\n",
      "Epoch 31/100\n",
      "1109/1109 [==============================] - 0s 151us/step - loss: 0.6636 - accuracy: 0.5753\n",
      "Epoch 32/100\n",
      "1109/1109 [==============================] - 0s 137us/step - loss: 0.6629 - accuracy: 0.5744\n",
      "Epoch 33/100\n",
      "1109/1109 [==============================] - 0s 134us/step - loss: 0.6623 - accuracy: 0.5753\n",
      "Epoch 34/100\n",
      "1109/1109 [==============================] - 0s 137us/step - loss: 0.6616 - accuracy: 0.5753\n",
      "Epoch 35/100\n",
      "1109/1109 [==============================] - 0s 134us/step - loss: 0.6609 - accuracy: 0.5753\n",
      "Epoch 36/100\n",
      "1109/1109 [==============================] - 0s 133us/step - loss: 0.6602 - accuracy: 0.5762\n",
      "Epoch 37/100\n",
      "1109/1109 [==============================] - 0s 130us/step - loss: 0.6596 - accuracy: 0.5744\n",
      "Epoch 38/100\n",
      "1109/1109 [==============================] - 0s 125us/step - loss: 0.6589 - accuracy: 0.5735\n",
      "Epoch 39/100\n",
      "1109/1109 [==============================] - 0s 126us/step - loss: 0.6582 - accuracy: 0.5726\n",
      "Epoch 40/100\n",
      "1109/1109 [==============================] - 0s 128us/step - loss: 0.6575 - accuracy: 0.5735\n",
      "Epoch 41/100\n",
      "1109/1109 [==============================] - 0s 125us/step - loss: 0.6569 - accuracy: 0.5771\n",
      "Epoch 42/100\n",
      "1109/1109 [==============================] - 0s 117us/step - loss: 0.6562 - accuracy: 0.5789\n",
      "Epoch 43/100\n",
      "1109/1109 [==============================] - 0s 122us/step - loss: 0.6555 - accuracy: 0.5789\n",
      "Epoch 44/100\n",
      "1109/1109 [==============================] - 0s 117us/step - loss: 0.6548 - accuracy: 0.5807\n",
      "Epoch 45/100\n",
      "1109/1109 [==============================] - 0s 119us/step - loss: 0.6541 - accuracy: 0.5834\n",
      "Epoch 46/100\n",
      "1109/1109 [==============================] - 0s 112us/step - loss: 0.6534 - accuracy: 0.5834\n",
      "Epoch 47/100\n",
      "1109/1109 [==============================] - 0s 111us/step - loss: 0.6527 - accuracy: 0.5816\n",
      "Epoch 48/100\n",
      "1109/1109 [==============================] - 0s 118us/step - loss: 0.6520 - accuracy: 0.5834\n",
      "Epoch 49/100\n",
      "1109/1109 [==============================] - 0s 116us/step - loss: 0.6513 - accuracy: 0.5825\n",
      "Epoch 50/100\n",
      "1109/1109 [==============================] - 0s 113us/step - loss: 0.6506 - accuracy: 0.5825\n",
      "Epoch 51/100\n",
      "1109/1109 [==============================] - 0s 107us/step - loss: 0.6498 - accuracy: 0.5843\n",
      "Epoch 52/100\n",
      "1109/1109 [==============================] - 0s 109us/step - loss: 0.6491 - accuracy: 0.5861\n",
      "Epoch 53/100\n",
      "1109/1109 [==============================] - 0s 108us/step - loss: 0.6484 - accuracy: 0.5870\n",
      "Epoch 54/100\n",
      "1109/1109 [==============================] - ETA: 0s - loss: 0.6503 - accuracy: 0.58 - 0s 107us/step - loss: 0.6476 - accuracy: 0.5888\n",
      "Epoch 55/100\n",
      "1109/1109 [==============================] - 0s 105us/step - loss: 0.6469 - accuracy: 0.5888\n",
      "Epoch 56/100\n",
      "1109/1109 [==============================] - 0s 105us/step - loss: 0.6461 - accuracy: 0.5897\n",
      "Epoch 57/100\n",
      "1109/1109 [==============================] - 0s 105us/step - loss: 0.6453 - accuracy: 0.5915\n",
      "Epoch 58/100\n",
      "1109/1109 [==============================] - 0s 104us/step - loss: 0.6445 - accuracy: 0.5933\n",
      "Epoch 59/100\n",
      "1109/1109 [==============================] - 0s 100us/step - loss: 0.6437 - accuracy: 0.5924\n",
      "Epoch 60/100\n",
      "1109/1109 [==============================] - 0s 104us/step - loss: 0.6429 - accuracy: 0.5888\n",
      "Epoch 61/100\n",
      "1109/1109 [==============================] - 0s 100us/step - loss: 0.6421 - accuracy: 0.5879\n",
      "Epoch 62/100\n",
      "1109/1109 [==============================] - 0s 100us/step - loss: 0.6413 - accuracy: 0.5870\n",
      "Epoch 63/100\n",
      "1109/1109 [==============================] - 0s 100us/step - loss: 0.6405 - accuracy: 0.5870\n",
      "Epoch 64/100\n",
      "1109/1109 [==============================] - 0s 100us/step - loss: 0.6396 - accuracy: 0.5870\n",
      "Epoch 65/100\n",
      "1109/1109 [==============================] - 0s 100us/step - loss: 0.6388 - accuracy: 0.5870\n",
      "Epoch 66/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6379 - accuracy: 0.5879\n",
      "Epoch 67/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6371 - accuracy: 0.5897\n",
      "Epoch 68/100\n",
      "1109/1109 [==============================] - 0s 94us/step - loss: 0.6362 - accuracy: 0.5915\n",
      "Epoch 69/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6353 - accuracy: 0.5951\n",
      "Epoch 70/100\n",
      "1109/1109 [==============================] - 0s 94us/step - loss: 0.6345 - accuracy: 0.5960\n",
      "Epoch 71/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6336 - accuracy: 0.5960\n",
      "Epoch 72/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6327 - accuracy: 0.5987\n",
      "Epoch 73/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6319 - accuracy: 0.5987\n",
      "Epoch 74/100\n",
      "1109/1109 [==============================] - 0s 99us/step - loss: 0.6310 - accuracy: 0.6005\n",
      "Epoch 75/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6301 - accuracy: 0.6032\n",
      "Epoch 76/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6293 - accuracy: 0.6014\n",
      "Epoch 77/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6284 - accuracy: 0.5996\n",
      "Epoch 78/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6275 - accuracy: 0.6032\n",
      "Epoch 79/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6267 - accuracy: 0.6069\n",
      "Epoch 80/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6258 - accuracy: 0.6050\n",
      "Epoch 81/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6250 - accuracy: 0.6050\n",
      "Epoch 82/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6241 - accuracy: 0.6060\n",
      "Epoch 83/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6233 - accuracy: 0.6041\n",
      "Epoch 84/100\n",
      "1109/1109 [==============================] - 0s 99us/step - loss: 0.6224 - accuracy: 0.6078\n",
      "Epoch 85/100\n",
      "1109/1109 [==============================] - 0s 102us/step - loss: 0.6216 - accuracy: 0.6105\n",
      "Epoch 86/100\n",
      "1109/1109 [==============================] - 0s 96us/step - loss: 0.6207 - accuracy: 0.6069\n",
      "Epoch 87/100\n",
      "1109/1109 [==============================] - 0s 100us/step - loss: 0.6199 - accuracy: 0.6078\n",
      "Epoch 88/100\n",
      "1109/1109 [==============================] - 0s 92us/step - loss: 0.6191 - accuracy: 0.6069\n",
      "Epoch 89/100\n",
      "1109/1109 [==============================] - 0s 91us/step - loss: 0.6182 - accuracy: 0.6069\n",
      "Epoch 90/100\n",
      "1109/1109 [==============================] - 0s 91us/step - loss: 0.6174 - accuracy: 0.6060\n",
      "Epoch 91/100\n",
      "1109/1109 [==============================] - 0s 93us/step - loss: 0.6166 - accuracy: 0.6069\n",
      "Epoch 92/100\n",
      "1109/1109 [==============================] - 0s 98us/step - loss: 0.6158 - accuracy: 0.6087\n",
      "Epoch 93/100\n",
      "1109/1109 [==============================] - 0s 98us/step - loss: 0.6150 - accuracy: 0.6096\n",
      "Epoch 94/100\n",
      "1109/1109 [==============================] - 0s 98us/step - loss: 0.6142 - accuracy: 0.6123\n",
      "Epoch 95/100\n",
      "1109/1109 [==============================] - 0s 101us/step - loss: 0.6135 - accuracy: 0.6141\n",
      "Epoch 96/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6127 - accuracy: 0.6177\n",
      "Epoch 97/100\n",
      "1109/1109 [==============================] - 0s 94us/step - loss: 0.6120 - accuracy: 0.6186\n",
      "Epoch 98/100\n",
      "1109/1109 [==============================] - 0s 129us/step - loss: 0.6113 - accuracy: 0.6195\n",
      "Epoch 99/100\n",
      "1109/1109 [==============================] - 0s 109us/step - loss: 0.6105 - accuracy: 0.6204\n",
      "Epoch 100/100\n",
      "1109/1109 [==============================] - 0s 98us/step - loss: 0.6099 - accuracy: 0.6222\n",
      "276/276 [==============================] - 0s 707us/step\n",
      "Epoch 1/100\n",
      "1109/1109 [==============================] - 2s 2ms/step - loss: 0.7237 - accuracy: 0.5077\n",
      "Epoch 2/100\n",
      "1109/1109 [==============================] - 0s 228us/step - loss: 0.7138 - accuracy: 0.5158\n",
      "Epoch 3/100\n",
      "1109/1109 [==============================] - 0s 227us/step - loss: 0.7068 - accuracy: 0.5239\n",
      "Epoch 4/100\n",
      "1109/1109 [==============================] - 0s 251us/step - loss: 0.7015 - accuracy: 0.5248\n",
      "Epoch 5/100\n",
      "1109/1109 [==============================] - 0s 257us/step - loss: 0.6976 - accuracy: 0.5266\n",
      "Epoch 6/100\n",
      "1109/1109 [==============================] - 0s 220us/step - loss: 0.6945 - accuracy: 0.5338\n",
      "Epoch 7/100\n",
      "1109/1109 [==============================] - 0s 238us/step - loss: 0.6920 - accuracy: 0.5410\n",
      "Epoch 8/100\n",
      "1109/1109 [==============================] - 0s 232us/step - loss: 0.6900 - accuracy: 0.5473\n",
      "Epoch 9/100\n",
      "1109/1109 [==============================] - 0s 236us/step - loss: 0.6882 - accuracy: 0.5509\n",
      "Epoch 10/100\n",
      "1109/1109 [==============================] - 0s 255us/step - loss: 0.6867 - accuracy: 0.5591\n",
      "Epoch 11/100\n",
      "1109/1109 [==============================] - 0s 180us/step - loss: 0.6853 - accuracy: 0.5564\n",
      "Epoch 12/100\n",
      "1109/1109 [==============================] - 0s 175us/step - loss: 0.6840 - accuracy: 0.5591\n",
      "Epoch 13/100\n",
      "1109/1109 [==============================] - 0s 176us/step - loss: 0.6828 - accuracy: 0.5582\n",
      "Epoch 14/100\n",
      "1109/1109 [==============================] - 0s 184us/step - loss: 0.6817 - accuracy: 0.5654\n",
      "Epoch 15/100\n",
      "1109/1109 [==============================] - 0s 171us/step - loss: 0.6806 - accuracy: 0.5645\n",
      "Epoch 16/100\n",
      "1109/1109 [==============================] - 0s 162us/step - loss: 0.6795 - accuracy: 0.5645\n",
      "Epoch 17/100\n",
      "1109/1109 [==============================] - 0s 159us/step - loss: 0.6785 - accuracy: 0.5627\n",
      "Epoch 18/100\n",
      "1109/1109 [==============================] - 0s 163us/step - loss: 0.6775 - accuracy: 0.5735\n",
      "Epoch 19/100\n",
      "1109/1109 [==============================] - 0s 213us/step - loss: 0.6765 - accuracy: 0.5780\n",
      "Epoch 20/100\n",
      "1109/1109 [==============================] - 0s 157us/step - loss: 0.6755 - accuracy: 0.5780\n",
      "Epoch 21/100\n",
      "1109/1109 [==============================] - 0s 160us/step - loss: 0.6746 - accuracy: 0.5843\n",
      "Epoch 22/100\n",
      "1109/1109 [==============================] - 0s 156us/step - loss: 0.6736 - accuracy: 0.5879\n",
      "Epoch 23/100\n",
      "1109/1109 [==============================] - 0s 151us/step - loss: 0.6725 - accuracy: 0.5915\n",
      "Epoch 24/100\n",
      "1109/1109 [==============================] - 0s 153us/step - loss: 0.6715 - accuracy: 0.5987\n",
      "Epoch 25/100\n",
      "1109/1109 [==============================] - 0s 131us/step - loss: 0.6705 - accuracy: 0.6014\n",
      "Epoch 26/100\n",
      "1109/1109 [==============================] - 0s 147us/step - loss: 0.6694 - accuracy: 0.5996\n",
      "Epoch 27/100\n",
      "1109/1109 [==============================] - 0s 133us/step - loss: 0.6683 - accuracy: 0.5969\n",
      "Epoch 28/100\n",
      "1109/1109 [==============================] - 0s 127us/step - loss: 0.6672 - accuracy: 0.6032\n",
      "Epoch 29/100\n",
      "1109/1109 [==============================] - 0s 136us/step - loss: 0.6661 - accuracy: 0.6041\n",
      "Epoch 30/100\n",
      "1109/1109 [==============================] - 0s 187us/step - loss: 0.6649 - accuracy: 0.6096\n",
      "Epoch 31/100\n",
      "1109/1109 [==============================] - 0s 168us/step - loss: 0.6638 - accuracy: 0.6150\n",
      "Epoch 32/100\n",
      "1109/1109 [==============================] - 0s 153us/step - loss: 0.6627 - accuracy: 0.6168\n",
      "Epoch 33/100\n",
      "1109/1109 [==============================] - 0s 139us/step - loss: 0.6615 - accuracy: 0.6186\n",
      "Epoch 34/100\n",
      "1109/1109 [==============================] - 0s 126us/step - loss: 0.6604 - accuracy: 0.6204\n",
      "Epoch 35/100\n",
      "1109/1109 [==============================] - 0s 119us/step - loss: 0.6593 - accuracy: 0.6240\n",
      "Epoch 36/100\n",
      "1109/1109 [==============================] - 0s 144us/step - loss: 0.6582 - accuracy: 0.6303\n",
      "Epoch 37/100\n",
      "1109/1109 [==============================] - 0s 159us/step - loss: 0.6570 - accuracy: 0.6375\n",
      "Epoch 38/100\n",
      "1109/1109 [==============================] - 0s 133us/step - loss: 0.6559 - accuracy: 0.6420\n",
      "Epoch 39/100\n",
      "1109/1109 [==============================] - 0s 109us/step - loss: 0.6548 - accuracy: 0.6447\n",
      "Epoch 40/100\n",
      "1109/1109 [==============================] - 0s 108us/step - loss: 0.6537 - accuracy: 0.6438\n",
      "Epoch 41/100\n",
      "1109/1109 [==============================] - 0s 109us/step - loss: 0.6527 - accuracy: 0.6447\n",
      "Epoch 42/100\n",
      "1109/1109 [==============================] - 0s 111us/step - loss: 0.6516 - accuracy: 0.6438\n",
      "Epoch 43/100\n",
      "1109/1109 [==============================] - 0s 112us/step - loss: 0.6505 - accuracy: 0.6447\n",
      "Epoch 44/100\n",
      "1109/1109 [==============================] - 0s 108us/step - loss: 0.6495 - accuracy: 0.6465\n",
      "Epoch 45/100\n",
      "1109/1109 [==============================] - 0s 109us/step - loss: 0.6484 - accuracy: 0.6447\n",
      "Epoch 46/100\n",
      "1109/1109 [==============================] - 0s 117us/step - loss: 0.6474 - accuracy: 0.6465\n",
      "Epoch 47/100\n",
      "1109/1109 [==============================] - 0s 122us/step - loss: 0.6464 - accuracy: 0.6483\n",
      "Epoch 48/100\n",
      "1109/1109 [==============================] - 0s 124us/step - loss: 0.6454 - accuracy: 0.6492\n",
      "Epoch 49/100\n",
      "1109/1109 [==============================] - 0s 130us/step - loss: 0.6445 - accuracy: 0.6483\n",
      "Epoch 50/100\n",
      "1109/1109 [==============================] - 0s 119us/step - loss: 0.6435 - accuracy: 0.6501\n",
      "Epoch 51/100\n",
      "1109/1109 [==============================] - 0s 115us/step - loss: 0.6426 - accuracy: 0.6510\n",
      "Epoch 52/100\n",
      "1109/1109 [==============================] - 0s 121us/step - loss: 0.6417 - accuracy: 0.6501\n",
      "Epoch 53/100\n",
      "1109/1109 [==============================] - 0s 116us/step - loss: 0.6408 - accuracy: 0.6501\n",
      "Epoch 54/100\n",
      "1109/1109 [==============================] - 0s 116us/step - loss: 0.6399 - accuracy: 0.6510\n",
      "Epoch 55/100\n",
      "1109/1109 [==============================] - 0s 118us/step - loss: 0.6391 - accuracy: 0.6519\n",
      "Epoch 56/100\n",
      "1109/1109 [==============================] - 0s 131us/step - loss: 0.6382 - accuracy: 0.6501\n",
      "Epoch 57/100\n",
      "1109/1109 [==============================] - 0s 132us/step - loss: 0.6374 - accuracy: 0.6465\n",
      "Epoch 58/100\n",
      "1109/1109 [==============================] - 0s 137us/step - loss: 0.6366 - accuracy: 0.6483\n",
      "Epoch 59/100\n",
      "1109/1109 [==============================] - 0s 141us/step - loss: 0.6358 - accuracy: 0.6465\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109/1109 [==============================] - 0s 148us/step - loss: 0.6351 - accuracy: 0.6447\n",
      "Epoch 61/100\n",
      "1109/1109 [==============================] - 0s 131us/step - loss: 0.6343 - accuracy: 0.6474\n",
      "Epoch 62/100\n",
      "1109/1109 [==============================] - 0s 106us/step - loss: 0.6336 - accuracy: 0.6456\n",
      "Epoch 63/100\n",
      "1109/1109 [==============================] - 0s 121us/step - loss: 0.6328 - accuracy: 0.6465\n",
      "Epoch 64/100\n",
      "1109/1109 [==============================] - 0s 124us/step - loss: 0.6321 - accuracy: 0.6492\n",
      "Epoch 65/100\n",
      "1109/1109 [==============================] - 0s 118us/step - loss: 0.6314 - accuracy: 0.6519\n",
      "Epoch 66/100\n",
      "1109/1109 [==============================] - 0s 173us/step - loss: 0.6308 - accuracy: 0.6546\n",
      "Epoch 67/100\n",
      "1109/1109 [==============================] - 0s 109us/step - loss: 0.6301 - accuracy: 0.6555\n",
      "Epoch 68/100\n",
      "1109/1109 [==============================] - 0s 103us/step - loss: 0.6295 - accuracy: 0.6619\n",
      "Epoch 69/100\n",
      "1109/1109 [==============================] - 0s 122us/step - loss: 0.6288 - accuracy: 0.6619\n",
      "Epoch 70/100\n",
      "1109/1109 [==============================] - 0s 137us/step - loss: 0.6282 - accuracy: 0.6610\n",
      "Epoch 71/100\n",
      "1109/1109 [==============================] - 0s 116us/step - loss: 0.6276 - accuracy: 0.6619\n",
      "Epoch 72/100\n",
      "1109/1109 [==============================] - 0s 107us/step - loss: 0.6270 - accuracy: 0.6610\n",
      "Epoch 73/100\n",
      "1109/1109 [==============================] - 0s 96us/step - loss: 0.6264 - accuracy: 0.6628\n",
      "Epoch 74/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6259 - accuracy: 0.6628\n",
      "Epoch 75/100\n",
      "1109/1109 [==============================] - 0s 94us/step - loss: 0.6253 - accuracy: 0.6646\n",
      "Epoch 76/100\n",
      "1109/1109 [==============================] - 0s 93us/step - loss: 0.6248 - accuracy: 0.6628\n",
      "Epoch 77/100\n",
      "1109/1109 [==============================] - 0s 115us/step - loss: 0.6243 - accuracy: 0.6637\n",
      "Epoch 78/100\n",
      "1109/1109 [==============================] - 0s 110us/step - loss: 0.6238 - accuracy: 0.6637\n",
      "Epoch 79/100\n",
      "1109/1109 [==============================] - 0s 105us/step - loss: 0.6233 - accuracy: 0.6664\n",
      "Epoch 80/100\n",
      "1109/1109 [==============================] - 0s 98us/step - loss: 0.6228 - accuracy: 0.6682\n",
      "Epoch 81/100\n",
      "1109/1109 [==============================] - 0s 166us/step - loss: 0.6223 - accuracy: 0.6655\n",
      "Epoch 82/100\n",
      "1109/1109 [==============================] - 0s 123us/step - loss: 0.6218 - accuracy: 0.6664\n",
      "Epoch 83/100\n",
      "1109/1109 [==============================] - 0s 99us/step - loss: 0.6213 - accuracy: 0.6664\n",
      "Epoch 84/100\n",
      "1109/1109 [==============================] - 0s 102us/step - loss: 0.6209 - accuracy: 0.6655\n",
      "Epoch 85/100\n",
      "1109/1109 [==============================] - 0s 106us/step - loss: 0.6204 - accuracy: 0.6664\n",
      "Epoch 86/100\n",
      "1109/1109 [==============================] - 0s 110us/step - loss: 0.6200 - accuracy: 0.6646\n",
      "Epoch 87/100\n",
      "1109/1109 [==============================] - 0s 105us/step - loss: 0.6196 - accuracy: 0.6655\n",
      "Epoch 88/100\n",
      "1109/1109 [==============================] - 0s 106us/step - loss: 0.6191 - accuracy: 0.6646\n",
      "Epoch 89/100\n",
      "1109/1109 [==============================] - 0s 102us/step - loss: 0.6187 - accuracy: 0.6655\n",
      "Epoch 90/100\n",
      "1109/1109 [==============================] - 0s 104us/step - loss: 0.6183 - accuracy: 0.6673\n",
      "Epoch 91/100\n",
      "1109/1109 [==============================] - 0s 104us/step - loss: 0.6179 - accuracy: 0.6664\n",
      "Epoch 92/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6175 - accuracy: 0.6682\n",
      "Epoch 93/100\n",
      "1109/1109 [==============================] - 0s 98us/step - loss: 0.6171 - accuracy: 0.6664\n",
      "Epoch 94/100\n",
      "1109/1109 [==============================] - 0s 104us/step - loss: 0.6167 - accuracy: 0.6673\n",
      "Epoch 95/100\n",
      "1109/1109 [==============================] - 0s 106us/step - loss: 0.6164 - accuracy: 0.6673\n",
      "Epoch 96/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6160 - accuracy: 0.6682\n",
      "Epoch 97/100\n",
      "1109/1109 [==============================] - 0s 97us/step - loss: 0.6156 - accuracy: 0.6682\n",
      "Epoch 98/100\n",
      "1109/1109 [==============================] - 0s 98us/step - loss: 0.6153 - accuracy: 0.6682\n",
      "Epoch 99/100\n",
      "1109/1109 [==============================] - 0s 108us/step - loss: 0.6149 - accuracy: 0.6682\n",
      "Epoch 100/100\n",
      "1109/1109 [==============================] - 0s 104us/step - loss: 0.6145 - accuracy: 0.6691\n",
      "276/276 [==============================] - 0s 638us/step\n"
     ]
    }
   ],
   "source": [
    "# build the scikit-learn interface for the keras model\n",
    "classifier = KerasClassifier(build_fn=build_model, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "# define the cross-validation iterator\n",
    "kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "# perform the k-fold cross-validation and store the scores in results\n",
    "results = cross_val_score(classifier, X, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy at fold  1  =  0.5467625856399536\n",
      "Test accuracy at fold  2  =  0.5143885016441345\n",
      "Test accuracy at fold  3  =  0.5415162444114685\n",
      "Test accuracy at fold  4  =  0.5652173757553101\n",
      "Test accuracy at fold  5  =  0.5036231875419617\n",
      "\n",
      "\n",
      "Final Cross-validation Test Accuracy: 0.5343015789985657\n",
      "Standard Deviation of Final Test Accuracy: 0.022364297103773998\n"
     ]
    }
   ],
   "source": [
    "# print accuracy for each fold\n",
    "for f in range(n_folds):\n",
    "    print(\"Test accuracy at fold \", f+1, \" = \", results[f])\n",
    "print(\"\\n\")\n",
    "# print overall cross-validation accuracy plus the standard deviation of the accuracies\n",
    "print(\"Final Cross-validation Test Accuracy:\", results.mean())\n",
    "print(\"Standard Deviation of Final Test Accuracy:\", results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1(activation='relu',optimizer='adam'):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(4,activation=activation,input_dim=X.shape[1]))\n",
    "    model.add(Dense(4,activation=activation))\n",
    "    model.add(Dense(4,activation=activation))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "def build_model_2(activation='relu', optimizer='adam'):\n",
    "    # create model 2\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=X.shape[1], activation=activation))\n",
    "    model.add(Dense(2, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# Create the function that returns the keras model 3\n",
    "def build_model_3(activation='relu', optimizer='adam'):\n",
    "    # create model 3\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=X.shape[1], activation=activation))\n",
    "    model.add(Dense(8, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Test Accuracy = 0.5090227663516999\n",
      "Model 2 Test Accuracy = 0.5183961093425751\n",
      "Model 3 Test Accuracy = 0.5111471056938172\n"
     ]
    }
   ],
   "source": [
    "results_1 = []\n",
    "# define the possible options for the model\n",
    "models = [build_model_1, build_model_2, build_model_3]\n",
    "# loop over models\n",
    "for m in range(len(models)):\n",
    "    # build the scikit-learn interface for the keras model\n",
    "    classifier = KerasClassifier(build_fn=models[m], epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    # define the cross-validation iterator\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    # perform the k-fold cross-validation and store the scores in result\n",
    "    result = cross_val_score(classifier, X, y, cv=kfold)\n",
    "    # add the scores to the results list \n",
    "    results_1.append(result)\n",
    "# Print cross-validation score for each model\n",
    "for m in range(len(models)):\n",
    "    print(\"Model\", m+1,\"Test Accuracy =\", results_1[m].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 10 , epochs = 100 , Test Accuracy = 0.530772352218628\n",
      "batch_size = 20 , epochs = 100 , Test Accuracy = 0.5141578018665314\n",
      "batch_size = 10 , epochs = 200 , Test Accuracy = 0.5089810788631439\n",
      "batch_size = 20 , epochs = 200 , Test Accuracy = 0.49383407831192017\n"
     ]
    }
   ],
   "source": [
    "# define a seed for random number generator so the result will be reproducible\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "# determine the number of folds for k-fold cross-validation\n",
    "n_folds = 5\n",
    "# define possible options for epochs and batch_size\n",
    "epochs = [100, 200]\n",
    "batches = [10, 20]\n",
    "# define the list to store cross-validation scores\n",
    "results_2 = []\n",
    "# loop over all possible pairs of epochs, batch_size\n",
    "for e in range(len(epochs)):\n",
    "    for b in range(len(batches)):\n",
    "        # build the scikit-learn interface for the keras model\n",
    "        classifier = KerasClassifier(build_fn=build_model_2, epochs=epochs[e], batch_size=batches[b], verbose=0)\n",
    "        # define the cross-validation iterator\n",
    "        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        # perform the k-fold cross-validation. \n",
    "        # store the scores in result\n",
    "        result = cross_val_score(classifier, X, y, cv=kfold)\n",
    "        # add the scores to the results list \n",
    "        results_2.append(result)\n",
    "# Print cross-validation score for each possible pair of epochs, batch_size\n",
    "c = 0\n",
    "for e in range(len(epochs)):\n",
    "    for b in range(len(batches)):\n",
    "        print(\"batch_size =\", batches[b],\", epochs =\", epochs[e], \", Test Accuracy =\", results_2[c].mean())\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a seed for random number generator so the result will be reproducible\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "# determine the number of folds for k-fold cross-validation, number of epochs and batch size\n",
    "n_folds = 5\n",
    "batch_size = 10\n",
    "epochs = 200\n",
    "# define the list to store cross-validation scores\n",
    "results_3 = []\n",
    "# define possible options for optimizer and activation\n",
    "optimizers = ['rmsprop', 'adam','sgd']\n",
    "activations = ['relu', 'tanh']\n",
    "# loop over all possible pairs of optimizer, activation\n",
    "for o in range(len(optimizers)):\n",
    "    for a in range(len(activations)):\n",
    "        optimizer = optimizers[o]\n",
    "        activation = activations[a]\n",
    "        # build the scikit-learn interface for the keras model\n",
    "        classifier = KerasClassifier(build_fn=build_model_2, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        # define the cross-validation iterator\n",
    "        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        # perform the k-fold cross-validation. \n",
    "        # store the scores in result\n",
    "        result = cross_val_score(classifier, X, y, cv=kfold)\n",
    "        # add the scores to the results list \n",
    "        results_3.append(result)\n",
    "# Print cross-validation score for each possible pair of optimizer, activation\n",
    "c = 0\n",
    "for o in range(len(optimizers)):\n",
    "    for a in range(len(activations)):\n",
    "        print(\"activation = \", activations[a],\", optimizer = \", optimizers[o], \", Test accuracy = \", results_3[c].mean())\n",
    "        c += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Regularization</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2# l1 is also there \n",
    "l2_param = 0.01\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(l2_param)))\n",
    "model_2.add(Dense(6, activation='relu', kernel_regularizer=l2(l2_param)))\n",
    "model_2.add(Dense(4, activation='relu', kernel_regularizer=l2(l2_param)))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# train the model using training set while evaluating on test set\n",
    "history=model_2.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=False)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper right')\n",
    "# print the best accuracy reached on the test set\n",
    "print(\"Best Accuracy on Validation Set =\", max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dropout</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a seed for random number generator so the result will be reproducible\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "from keras.layers import Dropout\n",
    "# create model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(Dense(10, activation='relu'))\n",
    "model_2.add(Dense(1))\n",
    "# Compile model\n",
    "model_2.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "# train the model using training set while evaluating on test set\n",
    "history=model_2.fit(X_train, y_train, batch_size = 50, epochs = 200, validation_data=(X_test, y_test), verbose=0, shuffle=False)\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylim((0, 25000))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper right')\n",
    "# print the best accuracy reached on the test set\n",
    "print(\"Lowest error on training set = \", min(history.history['loss']))\n",
    "print(\"Lowest error on validation set = \", min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Hyperparameter Tune</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function that returns the keras model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "def build_model(lambda_parameter):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=X.shape[1], activation='relu', kernel_regularizer=l2(lambda_parameter)))\n",
    "    model.add(Dense(6, activation='relu', kernel_regularizer=l2(lambda_parameter)))\n",
    "    model.add(Dense(4, activation='relu', kernel_regularizer=l2(lambda_parameter)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# define a seed for random number generator so the result will be reproducible\n",
    "import numpy as np\n",
    "from tensorflow import random\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "# create the Keras wrapper with scikit learn\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)\n",
    "# define all the possible values for each hyperparameter\n",
    "lambda_parameter = [0.01, 0.5, 1]\n",
    "epochs = [50, 100]\n",
    "batch_size = [20]\n",
    "# create the dictionary containing all possible values of hyperparameters\n",
    "param_grid = dict(lambda_parameter=lambda_parameter, epochs=epochs, batch_size=batch_size)\n",
    "# perform 5-fold cross-validation for ??????? store the results\n",
    "grid_seach = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "results_1 = grid_seach.fit(X, y)\n",
    "print(\"Best cross-validation score =\", results_1.best_score_)\n",
    "print(\"Parameters for Best cross-validation score=\", results_1.best_params_)\n",
    "# print the results for all evaluated hyperparameter combinations\n",
    "accuracy_means = results_1.cv_results_['mean_test_score']\n",
    "accuracy_stds = results_1.cv_results_['std_test_score']\n",
    "parameters = results_1.cv_results_['params']\n",
    "for p in range(len(parameters)):\n",
    "    print(\"Accuracy %f (std %f) for params %r\" % (accuracy_means[p], accuracy_stds[p], parameters[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-635f88d2f03c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Transform the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Transform the testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#------------------------Standard Scaler-----------------\n",
    "# Initialize StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# Transform the training data\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=X_test.columns)\n",
    "# Transform the testing data\n",
    "X_test=sc.fit_transform(X_test)\n",
    "X_test=pd.DataFrame(X_test,columns=X_train.column)\n",
    "# What is kernelInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ROC</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(X_test)\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(\"ROC Curve for APS Failure\")\n",
    "plt.xlabel(\"False Positive rate (1-Specificity)\")\n",
    "plt.ylabel(\"True Positive rate (Sensitivity)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
